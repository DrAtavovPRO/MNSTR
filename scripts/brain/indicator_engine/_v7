#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Улучшенный модуль Indicator Engine для проекта MONSTER Trading System.
Реализует расширяемую архитектуру в стиле конструктора с глубоким контекстуальным анализом индикаторов.
"""

# ============================================================
# ИМПОРТЫ И ОПРЕДЕЛЕНИЯ ПУТЕЙ
# ============================================================

# Стандартные библиотеки Python
import os
import sys
import time
import json
import logging
import traceback
import functools
import concurrent.futures
from typing import Dict, List, Tuple, Optional, Union, Any, Set, Callable
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, namedtuple

# Определение путей для корректного импорта
current_file = os.path.abspath(__file__)
brain_dir = os.path.dirname(current_file)
core_dir = os.path.dirname(brain_dir)
scripts_dir = os.path.dirname(core_dir)
project_root = os.path.dirname(scripts_dir)

# Добавляем корневую директорию проекта в sys.path
sys.path.insert(0, project_root)

# Импорт научных библиотек с проверкой их наличия
try:
    import pandas as pd
    import numpy as np
    from scipy import signal
    from scipy.signal import argrelextrema
    from scipy import stats
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("ОШИБКА: pandas, numpy или scipy не установлены!")
    sys.exit(1)

# Импорт компонентов системы с обработкой возможных ошибок
try:
    # Импорт утилит логирования и мониторинга
    sys.path.append(os.path.join(project_root, 'scripts'))
    from utils.brain_logger import get_logger, send_telegram, log_execution_time
    from utils.monitor_system import monitor_system
    
    # Импорт конфигурационных параметров
    from config.brain.config import (
        # Базовые пути и настройки
        PATHS,
        TIMEFRAMES,
        PROCESSING,
        
        # Параметры для Indicator Engine
        INDICATOR_ENGINE,
        
        # Настройки индикаторов и паттернов
        INDICATORS,
        PATTERNS,
        DIVERGENCES,
        
        # Дополнительные настройки для торговых сигналов
        ORDERBOOK_ANALYSIS
    )
    
    LOGGER_IMPORTED = True
    
except ImportError as e:
    LOGGER_IMPORTED = False
    import logging
    logging.basicConfig(level=logging.ERROR)
    log_console = logging.getLogger("bootstrap")
    log_console.error(f"КРИТИЧЕСКАЯ ОШИБКА: Не удалось импортировать необходимые модули: {e}")
    log_console.error(f"Детали исключения: {traceback.format_exc()}")
    
# ============================================================
# НАСТРОЙКА ЛОГИРОВАНИЯ (ВСТАВИТЬ В НАЧАЛО ФАЙЛА)
# ============================================================

# ============================================================
# НАСТРОЙКА ЦЕНТРАЛИЗОВАННОГО ЛОГИРОВАНИЯ
# ============================================================

def setup_centralized_logging():
    """
    Настройка централизованной системы логирования.
    Создает единую структуру логов для всего приложения.
    """
    # Сброс всех существующих настроек логирования
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # Установка базового уровня логирования
    logging.root.setLevel(logging.DEBUG)
    
    # Создание директории для логов
    log_dir = os.path.join('logs', 'brain', 'indicators')
    os.makedirs(log_dir, exist_ok=True)
    
    # Форматтеры для разных выводов
    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s: %(message)s')
    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s [%(filename)s:%(lineno)d]: %(message)s')
    
    # Консольный вывод (INFO)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    logging.root.addHandler(console_handler)
    
    # Общий файл логов (DEBUG)
    general_handler = logging.FileHandler(os.path.join(log_dir, 'indicator_engine.log'))
    general_handler.setLevel(logging.DEBUG)
    general_handler.setFormatter(file_formatter)
    logging.root.addHandler(general_handler)
    
    # Файл логов ошибок (ERROR)
    error_handler = logging.FileHandler(os.path.join(log_dir, 'errors.log'))
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(file_formatter)
    logging.root.addHandler(error_handler)

# Запускаем настройку логирования при импорте модуля
setup_centralized_logging()

# Функция для получения логгера без дополнительных обработчиков
def get_clean_logger(name):
    """
    Получает логгер для модуля без добавления дополнительных обработчиков.
    Использует настройки корневого логгера.
    
    Args:
        name: Имя логгера
        
    Returns:
        Настроенный логгер
    """
    return logging.getLogger(name)

# Основной логгер модуля
logger = get_clean_logger("indicator_engine")

# ============================================================
# КОНСТАНТЫ И ТИПЫ ДАННЫХ
# ============================================================

# Определение структуры для результатов анализа индикатора
IndicatorAnalysisResult = namedtuple('IndicatorAnalysisResult', [
    'indicator_name',           # Имя индикатора
    'current_value',            # Текущее значение
    'historical_context',       # Словарь с историческим контекстом
    'market_phases',            # Словарь с анализом по фазам рынка
    'signals',                  # Список сигналов
    'statistics',               # Словарь со статистикой
    'recommendation',           # Рекомендация (buy, sell, neutral)
    'confidence',               # Уверенность рекомендации (0.0 - 1.0)
    'metadata'                  # Дополнительные метаданные
])

# Типы рыночных условий
class MarketPhase(Enum):
    UPTREND = "uptrend"
    DOWNTREND = "downtrend"
    SIDEWAYS = "sideways"
    ACCUMULATION = "accumulation"
    DISTRIBUTION = "distribution"
    VOLATILE = "volatile"
    STABLE = "stable"
    UNKNOWN = "unknown"

# Типы сигналов
class SignalType(Enum):
    BUY = "buy"
    SELL = "sell"
    NEUTRAL = "neutral"
    STRONG_BUY = "strong_buy"
    STRONG_SELL = "strong_sell"
    EXIT = "exit"
    HOLD = "hold"

# ============================================================
# АБСТРАКТНЫЕ КЛАССЫ И ИНТЕРФЕЙСЫ
# ============================================================

class BaseIndicator(ABC):
    """
    Базовый абстрактный класс для всех индикаторов.
    Определяет интерфейс и общую функциональность для всех индикаторов.
    """
    
    def __init__(self, params: Dict = None, name: str = None):
        """
        Инициализирует индикатор.
        
        Args:
            params: Словарь с параметрами индикатора
            name: Имя индикатора (если не указано, используется имя класса)
        """
        self.params = params or {}
        self.name = name or self.__class__.__name__
        self.logger = get_clean_logger(f"indicator.{self.name.lower()}")
        
    @abstractmethod
    def calculate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Рассчитывает индикатор и добавляет его в DataFrame.
        
        Args:
            df: DataFrame с данными (OHLCV)
            
        Returns:
            DataFrame с добавленными значениями индикатора
        """
        pass
    
    @abstractmethod
    def generate_signals(self, df: pd.DataFrame) -> List[Dict]:
        """
        Генерирует торговые сигналы на основе индикатора.
        
        Args:
            df: DataFrame с данными и рассчитанным индикатором
            
        Returns:
            Список словарей с сигналами
        """
        pass
    
    @abstractmethod
    def analyze_context(self, df: pd.DataFrame, timeframe: str) -> IndicatorAnalysisResult:
        """
        Выполняет контекстуальный анализ индикатора на основе всей истории.
        
        Args:
            df: DataFrame с данными и рассчитанным индикатором
            timeframe: Таймфрейм анализа ('5m', '15m', '1h', и т.д.)
            
        Returns:
            IndicatorAnalysisResult с результатами анализа
        """
        pass
    
    def get_cache_key(self, symbol: str, timeframe: str) -> str:
        """
        Создает уникальный ключ для кеширования результатов.
        
        Args:
            symbol: Символ монеты
            timeframe: Таймфрейм
            
        Returns:
            Строка-ключ для кеширования
        """
        param_hash = hash(frozenset(self.params.items())) if self.params else 0
        return f"{symbol}_{timeframe}_{self.name}_{param_hash}"
    
    def detect_market_phase(self, df: pd.DataFrame) -> MarketPhase:
        """
        Определяет текущую фазу рынка на основе исторических данных.
        
        Args:
            df: DataFrame с данными OHLCV
            
        Returns:
            MarketPhase: Определенная фаза рынка
        """
        # ВРЕМЕННО ОТКЛЮЧЕНО - возвращаем Unknown для всех случаев
        self.logger.debug("Определение фазы рынка временно отключено")
        return MarketPhase.UNKNOWN
    
    def evaluate_indicator_performance(self, df: pd.DataFrame, indicator_column: str) -> Dict:
        """
        Оценивает историческую эффективность индикатора.
        
        Args:
            df: DataFrame с данными и рассчитанным индикатором
            indicator_column: Имя столбца с индикатором
            
        Returns:
            Dict: Словарь со статистикой эффективности
        """
        try:
            if indicator_column not in df.columns or len(df) < 30:
                return {"status": "insufficient_data"}
            
            # Базовая статистика
            stats_data = {
                "mean": float(df[indicator_column].mean()),
                "median": float(df[indicator_column].median()),
                "std": float(df[indicator_column].std()),
                "min": float(df[indicator_column].min()),
                "max": float(df[indicator_column].max()),
                "current_percentile": float(stats.percentileofscore(
                    df[indicator_column].dropna(), df[indicator_column].iloc[-1]
                )),
                "status": "success"
            }
            
            return stats_data
            
        except Exception as e:
            self.logger.error(f"Ошибка при оценке производительности индикатора: {e}")
            return {"status": "error", "message": str(e)}


class IndicatorRegistry:
    """
    Реестр индикаторов для централизованного управления и доступа к индикаторам.
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(IndicatorRegistry, cls).__new__(cls)
            cls._instance.indicators = {}
            cls._instance.logger = get_clean_logger("indicator_registry")
        return cls._instance
    
    def register(self, indicator_class: type, name: str = None, params: Dict = None) -> None:
        """
        Регистрирует индикатор в реестре.
        
        Args:
            indicator_class: Класс индикатора
            name: Имя для регистрации (если None, используется имя класса)
            params: Параметры для инициализации индикатора
        """
        name = name or indicator_class.__name__
        
        try:
            indicator_instance = indicator_class(params=params, name=name)
            self.indicators[name] = indicator_instance
            self.logger.info(f"Индикатор {name} успешно зарегистрирован")
        except Exception as e:
            self.logger.error(f"Ошибка при регистрации индикатора {name}: {e}")
    
    def get(self, name: str) -> Optional[BaseIndicator]:
        """
        Получает индикатор по имени.
        
        Args:
            name: Имя индикатора
            
        Returns:
            Экземпляр индикатора или None, если индикатор не найден
        """
        return self.indicators.get(name)
    
    def get_all(self) -> Dict[str, BaseIndicator]:
        """
        Получает все зарегистрированные индикаторы.
        
        Returns:
            Словарь с индикаторами {имя: экземпляр}
        """
        return self.indicators
    
    def has_indicator(self, name: str) -> bool:
        """
        Проверяет, зарегистрирован ли индикатор.
        
        Args:
            name: Имя индикатора
            
        Returns:
            True, если индикатор зарегистрирован, иначе False
        """
        return name in self.indicators


# ============================================================
# УТИЛИТЫ КЕШИРОВАНИЯ, ХРАНЕНИЯ И ОБРАБОТКИ
# ============================================================

class DataCache:
    """
    Класс для кеширования результатов расчетов.
    Поддерживает кеширование в памяти и опционально в Redis.
    """
    
    def __init__(self, use_redis: bool = False, redis_config: Dict = None):
        """
        Инициализирует кеш данных.
        
        Args:
            use_redis: Использовать ли Redis для кеширования
            redis_config: Конфигурация Redis (хост, порт, база данных)
        """
        self.memory_cache = {}
        self.use_redis = use_redis
        self.logger = get_clean_logger("data_storage")
        
        if use_redis:
            try:
                import redis
                redis_config = redis_config or {}
                # Подключение к Redis
                self.redis_client = redis.Redis(
                    host=redis_config.get('host', 'localhost'),
                    port=redis_config.get('port', 6379),
                    db=redis_config.get('db', 0)
                )
                self.logger.info("Redis подключен успешно")
            except ImportError:
                self.logger.warning("Модуль redis не установлен. Используется только кеширование в памяти.")
                self.use_redis = False
            except Exception as e:
                self.logger.error(f"Ошибка при подключении к Redis: {e}")
                self.use_redis = False
    
    def get(self, key: str) -> Any:
        """
        Получает данные из кеша.
        
        Args:
            key: Ключ для поиска в кеше
            
        Returns:
            Значение из кеша или None, если ключа нет
        """
        # Сначала проверяем в памяти
        if key in self.memory_cache:
            return self.memory_cache[key]
        
        # Затем проверяем в Redis, если он используется
        if self.use_redis:
            try:
                import pickle
                data = self.redis_client.get(key)
                if data:
                    return pickle.loads(data)
            except Exception as e:
                self.logger.error(f"Ошибка при получении данных из Redis: {e}")
        
        return None
    
    def set(self, key: str, value: Any, ttl: int = None) -> None:
        """
        Сохраняет данные в кеш.
        
        Args:
            key: Ключ для сохранения в кеше
            value: Значение для сохранения
            ttl: Время жизни в секундах (только для Redis)
        """
        # Сохраняем в памяти
        self.memory_cache[key] = value
        
        # Сохраняем в Redis, если он используется
        if self.use_redis:
            try:
                import pickle
                self.redis_client.set(key, pickle.dumps(value))
                if ttl:
                    self.redis_client.expire(key, ttl)
            except Exception as e:
                self.logger.error(f"Ошибка при сохранении данных в Redis: {e}")


class DataStorage:
    """
    Класс для работы с хранилищем данных.
    Поддерживает сохранение в JSON и опционально в Parquet.
    """
    
    def __init__(self, base_path: str, use_parquet: bool = False):
        """
        Инициализирует хранилище данных.
        
        Args:
            base_path: Базовый путь к данным
            use_parquet: Использовать ли Parquet для хранения данных
        """
        self.base_path = base_path
        self.use_parquet = use_parquet
        self.logger = get_clean_logger("data_storage")
        
        if use_parquet:
            try:
                import pyarrow as pa
                import pyarrow.parquet as pq
                self.pa = pa
                self.pq = pq
                self.logger.info("Parquet поддержка включена")
            except ImportError:
                self.logger.warning("Модули pyarrow и/или pyarrow.parquet не установлены. Используется JSON.")
                self.use_parquet = False
    
    def save_indicators(self, symbol: str, timeframe: str, data: Dict) -> None:
        """
        Сохраняет результаты расчета индикаторов.
        
        Args:
            symbol: Символ монеты
            timeframe: Таймфрейм
            data: Словарь с результатами расчета индикаторов
        """
        # Создаем директорию, если она не существует
        indicators_dir = os.path.join(self.base_path, "indicators", "current", timeframe)
        os.makedirs(indicators_dir, exist_ok=True)
        
        # Путь к файлу результатов
        file_path = os.path.join(indicators_dir, f"{symbol}_{timeframe}")
        
        if self.use_parquet:
            try:
                # Преобразование в формат, подходящий для Parquet
                flat_data = self._flatten_dict(data)
                table = self.pa.Table.from_pydict(flat_data)
                self.pq.write_table(table, f"{file_path}.parquet")
                self.logger.info(f"Данные сохранены в Parquet: {file_path}.parquet")
            except Exception as e:
                self.logger.error(f"Ошибка при сохранении данных в Parquet: {e}")
                # Если возникла ошибка, сохраняем в JSON как запасной вариант
                self._save_to_json(f"{file_path}.json", data)
        else:
            # Сохранение в JSON
            self._save_to_json(f"{file_path}.json", data)
    
    def _save_to_json(self, file_path: str, data: Dict) -> None:
        """
        Сохраняет данные в JSON файл.
        
        Args:
            file_path: Путь к файлу
            data: Данные для сохранения
        """
        try:
            # Создаем директорию, если она не существует
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # Сохраняем данные в JSON
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, default=self._json_serializer)
            
            self.logger.info(f"Данные успешно сохранены в {file_path}")
        except Exception as e:
            self.logger.error(f"Ошибка при сохранении в JSON: {e}")
            self.logger.debug(traceback.format_exc())

    def _json_serializer(self, obj):
        """
        Сериализатор для JSON, который может обрабатывать нестандартные типы.
        """
        if isinstance(obj, (pd.Timestamp, datetime)):
            return obj.isoformat()
        elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Series):
            return obj.to_dict()
        elif isinstance(obj, pd.DataFrame):
            return obj.to_dict(orient='records')
        elif isinstance(obj, Enum):
            return obj.value
        elif hasattr(obj, '__dict__'):
            return obj.__dict__
        else:
            return str(obj)
    
    def load_indicators(self, symbol: str, timeframe: str) -> Optional[Dict]:
        """
        Загружает сохраненные результаты индикаторов.
        
        Args:
            symbol: Символ монеты
            timeframe: Таймфрейм
            
        Returns:
            Словарь с результатами расчета индикаторов или None, если файл не найден
        """
        # Путь к файлу результатов
        indicators_dir = os.path.join(self.base_path, "indicators", "current", timeframe)
        file_path = os.path.join(indicators_dir, f"{symbol}_{timeframe}")
        
        try:
            if self.use_parquet and os.path.exists(f"{file_path}.parquet"):
                # Загрузка из Parquet
                table = self.pq.read_table(f"{file_path}.parquet")
                flat_dict = {col: table[col].to_pylist() for col in table.column_names}
                self.logger.info(f"Данные успешно загружены из Parquet: {file_path}.parquet")
                return self._unflatten_dict(flat_dict)
            elif os.path.exists(f"{file_path}.json"):
                # Загрузка из JSON
                with open(f"{file_path}.json", 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.logger.info(f"Данные успешно загружены из JSON: {file_path}.json")
                return data
            else:
                self.logger.debug(f"Файл с индикаторами для {symbol} ({timeframe}) не найден")
                return None
        except Exception as e:
            self.logger.error(f"Ошибка при загрузке индикаторов для {symbol} ({timeframe}): {e}")
            return None
    
    def _flatten_dict(self, d: Dict, parent_key: str = '') -> Dict:
        """
        Преобразует вложенный словарь в плоскую структуру для сохранения в Parquet.
        
        Args:
            d: Вложенный словарь
            parent_key: Ключ родительского элемента
            
        Returns:
            Плоский словарь с составными ключами
        """
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}.{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(self._flatten_dict(v, new_key).items())
            else:
                items.append((new_key, v))
        return dict(items)
    
    def _unflatten_dict(self, d: Dict) -> Dict:
        """
        Преобразует плоскую структуру обратно во вложенный словарь после загрузки из Parquet.
        
        Args:
            d: Плоский словарь с составными ключами
            
        Returns:
            Вложенный словарь
        """
        result = {}
        for key, value in d.items():
            parts = key.split('.')
            current = result
            for part in parts[:-1]:
                if part not in current:
                    current[part] = {}
                current = current[part]
            current[parts[-1]] = value
        return result


class ParallelProcessor:
    """
    Класс для параллельной обработки данных с использованием ThreadPoolExecutor.
    """
    
    def __init__(self, max_workers: int = None, chunk_size: int = None):
        """
        Инициализирует процессор.
        
        Args:
            max_workers: Максимальное количество рабочих потоков
            chunk_size: Размер пакета для обработки
        """
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        self.logger = get_clean_logger("parallel_processor")
    
    def process_symbols(self, symbols: List[str], process_func, **kwargs) -> Dict:
        """
        Обрабатывает список символов параллельно.
        
        Args:
            symbols: Список символов для обработки
            process_func: Функция для обработки пакета символов
            **kwargs: Дополнительные параметры для функции обработки
            
        Returns:
            Словарь с результатами обработки
        """
        start_time = time.time()
        symbols_count = len(symbols)
        
        self.logger.info(f"Начало параллельной обработки {symbols_count} символов")
        
        # Подготовка чанков для обработки
        if self.chunk_size and symbols_count > self.chunk_size:
            chunks = [symbols[i:i+self.chunk_size] for i in range(0, symbols_count, self.chunk_size)]
            self.logger.info(f"Разделение на {len(chunks)} пакетов по {self.chunk_size} символов")
        else:
            chunks = [symbols]
            self.logger.info(f"Обработка всех символов в одном пакете")
        
        # Определение оптимального числа рабочих потоков
        workers = min(self.max_workers or os.cpu_count(), len(chunks))
        self.logger.info(f"Доступно {os.cpu_count()} процессорных ядер, используется {workers} {'поток' if workers == 1 else 'потоков'}")
        
        results = {
            'success': True,
            'total_symbols': len(symbols),
            'successful_symbols': 0,
            'failed_symbols': 0,
            'total_signals': 0,
            'results': {}
        }
        
        self.logger.info(f"Начало параллельной обработки {len(symbols)} символов в {len(chunks)} пакетах")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            for chunk in chunks:
                future = executor.submit(process_func, chunk, **kwargs)
                futures.append(future)
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    chunk_result = future.result()
                    
                    # Объединение результатов
                    results['successful_symbols'] += chunk_result.get('successful_symbols', 0)
                    results['failed_symbols'] += chunk_result.get('failed_symbols', 0)
                    results['total_signals'] += chunk_result.get('total_signals', 0)
                    
                    if 'results' in chunk_result:
                        results['results'].update(chunk_result['results'])
                        
                except Exception as e:
                    self.logger.error(f"Ошибка при обработке чанка: {e}")
                    self.logger.debug(traceback.format_exc())
                    results['success'] = False
        
        execution_time = time.time() - start_time
        results['execution_time'] = execution_time
        
        self.logger.info(f"Параллельная обработка завершена за {execution_time:.2f} сек. "
                         f"Успешно: {results['successful_symbols']}/{results['total_symbols']}")
        
        return results


# ============================================================
# РЕАЛИЗАЦИЯ RSI С КОНТЕКСТУАЛЬНЫМ АНАЛИЗОМ
# ============================================================

class RSI(BaseIndicator):
    """
    Индикатор Relative Strength Index (RSI) с расширенным контекстуальным анализом.
    
    RSI измеряет скорость и изменение ценовых движений, анализируя соотношение средних
    положительных и отрицательных изменений цены за определенный период.
    
    Расширенный анализ включает:
    - Анализ исторических уровней RSI
    - Оценку эффективности сигналов RSI в различных рыночных условиях
    - Анализ дивергенций RSI и цены
    - Адаптивные уровни перекупленности/перепроданности на основе исторических данных
    """
    
    def __init__(self, params: Dict = None, name: str = None):
        """
        Инициализирует индикатор RSI с расширенными возможностями анализа.
        
        Args:
            params: Словарь с параметрами индикатора
            name: Имя индикатора (если не указано, используется имя класса)
        """
        # Вызываем конструктор родительского класса
        super().__init__(params, name)
        
        # Инициализируем параметры
        self.period = self.params.get('period', 14)
        self.overbought = self.params.get('overbought', 70)
        self.oversold = self.params.get('oversold', 30)
        self.adaptive_levels = self.params.get('adaptive_levels', True)
        self.context_window = self.params.get('context_window', 90)
        
        # Логируем параметры для отладки
        self.logger.debug(f"RSI инициализирован с параметрами: period={self.period}, "
                         f"overbought={self.overbought}, oversold={self.oversold}, "
                         f"adaptive_levels={self.adaptive_levels}")
    
    def _get_quartile(self, df: pd.DataFrame, value: float) -> int:
        """
        Определяет квартиль, в котором находится значение RSI относительно исторических данных.
        
        Args:
            df: DataFrame с данными
            value: Текущее значение RSI
            
        Returns:
            int: Номер квартиля (1, 2, 3, или 4)
        """
        rsi_values = df['rsi'].dropna()
        current_percentile = stats.percentileofscore(rsi_values, value)
        
        if current_percentile < 25:
            return 1
        elif current_percentile < 50:
            return 2
        elif current_percentile < 75:
            return 3
        else:
            return 4
        
    def _ensure_min_adaptive_range(self, overbought: float, oversold: float, min_range: float = 20.0) -> Tuple[float, float]:
        """
        Обеспечивает минимальный разброс между адаптивными уровнями перекупленности/перепроданности.
        
        Args:
            overbought: Адаптивный уровень перекупленности
            oversold: Адаптивный уровень перепроданности
            min_range: Минимальный разброс между уровнями
            
        Returns:
            Tuple[float, float]: (новый_уровень_перекупленности, новый_уровень_перепроданности)
        """
        if overbought - oversold < min_range:
            mid_point = (overbought + oversold) / 2
            new_overbought = mid_point + min_range / 2
            new_oversold = mid_point - min_range / 2
            self.logger.debug(f"Адаптивные уровни расширены: {oversold:.2f}/{overbought:.2f} -> {new_oversold:.2f}/{new_overbought:.2f}")
            return new_overbought, new_oversold
        
        return overbought, oversold

    def calculate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Рассчитывает RSI по методу Уайлдера и добавляет его в DataFrame.
        
        Формула:
            RSI = 100 - (100 / (1 + RS))
            где RS = Средний прирост / Средние потери
            
        Метод Уайлдера для расчета среднего:
            Первое значение = SMA за период
            Последующие значения = (Пред. значение * (период-1) + Текущее значение) / период
        
        Args:
            df: DataFrame с данными (должен содержать столбец 'close')
            
        Returns:
            DataFrame с добавленным столбцом 'rsi'
        """
        self.logger.info(f"Начинаем расчет RSI по методу Уайлдера с периодом {self.period}")
        self.logger.debug(f"Размер входного DataFrame: {len(df)} строк, столбцы: {list(df.columns)}")
        
        start_time = time.time()

        if 'close' not in df.columns:
            self.logger.error("DataFrame не содержит столбец 'close', необходимый для расчета RSI")
            raise ValueError("DataFrame должен содержать столбец 'close' для расчета RSI")
        
        try:
            # Логируем первые несколько значений цены закрытия
            self.logger.debug(f"Первые 5 значений цены закрытия: {df['close'].head(5).tolist()}")
            
            # Расчет изменений цены закрытия
            self.logger.debug("Расчет изменений цены закрытия (close_delta)")
            close_delta = df['close'].diff()
            self.logger.debug(f"Первые 5 значений close_delta: {close_delta.head(5).tolist()}")
            
            # Получаем положительные и отрицательные изменения
            self.logger.debug("Разделение на положительные и отрицательные изменения")
            up = close_delta.where(close_delta > 0, 0)
            down = -close_delta.where(close_delta < 0, 0)
            
            # Логируем первые несколько значений up и down
            self.logger.debug(f"Первые 5 значений up: {up.head(5).tolist()}")
            self.logger.debug(f"Первые 5 значений down: {down.head(5).tolist()}")
            
            # Инициализируем массивы для средних значений
            avg_gain = pd.Series(index=df.index)
            avg_loss = pd.Series(index=df.index)
            
            # Метод Уайлдера: расчет первых значений как SMA
            if len(df) >= self.period:
                self.logger.debug(f"Расчет первых значений gain/loss как SMA с периодом {self.period}")
                first_avg_gain = up.iloc[:self.period].mean()
                first_avg_loss = down.iloc[:self.period].mean()
                
                # Устанавливаем первое значение
                avg_gain.iloc[self.period-1] = first_avg_gain
                avg_loss.iloc[self.period-1] = first_avg_loss
                
                # Метод Уайлдера: итеративное вычисление последующих значений
                self.logger.debug("Расчет последующих значений gain/loss по формуле Уайлдера")
                for i in range(self.period, len(df)):
                    avg_gain.iloc[i] = (avg_gain.iloc[i-1] * (self.period-1) + up.iloc[i]) / self.period
                    avg_loss.iloc[i] = (avg_loss.iloc[i-1] * (self.period-1) + down.iloc[i]) / self.period
                
                # Защита от деления на ноль с минимальным искажением
                epsilon = 1e-10  # Меньшее значение для минимального искажения
                self.logger.debug("Расчет Relative Strength (RS)")
                rs = avg_gain / avg_loss.replace(0, epsilon)
                
                # Рассчитываем RSI
                self.logger.debug("Расчет RSI на основе RS")
                df['rsi'] = 100 - (100 / (1 + rs))
                
                # Логируем первые и последние значения RSI
                self.logger.debug(f"Первые 5 ненулевых значений RSI: {df['rsi'].dropna().head(5).tolist()}")
                self.logger.debug(f"Последние 5 значений RSI: {df['rsi'].tail(5).tolist()}")
                
                # Если используются адаптивные уровни, рассчитываем их
                if self.adaptive_levels and len(df) > 30:
                    self.logger.debug("Расчет адаптивных уровней RSI")
                    
                    # Определяем размер окна в зависимости от таймфрейма
                    # Оптимизированные проценты для каждого таймфрейма согласно рекомендациям
                    timeframe_percentages = {
                        '1h': 0.30,   # 30% для внутридневной торговли - баланс между чувствительностью и стабильностью
                        '4h': 0.40,   # 40% для среднесрочного контекста
                        '24h': 0.50,  # 50% для надежных долгосрочных уровней
                        '7d': 0.60    # 60% для максимально достоверных глобальных уровней
                    }
                    
                    # Определяем таймфрейм из данных или имени файла
                    timeframe = None
                    if hasattr(df, 'timeframe'):
                        timeframe = getattr(df, 'timeframe')
                    elif hasattr(df, 'symbol') and '_' in getattr(df, 'symbol'):
                        # Пробуем извлечь таймфрейм из имени символа (например, "BTC_1h")
                        parts = getattr(df, 'symbol').split('_')
                        if len(parts) > 1 and parts[1] in timeframe_percentages:
                            timeframe = parts[1]
                    
                    # Если таймфрейм не удалось определить, используем процент по умолчанию
                    if timeframe in timeframe_percentages:
                        window_percentage = timeframe_percentages[timeframe]
                        self.logger.debug(f"Используем {window_percentage*100}% истории для таймфрейма {timeframe}")
                    else:
                        # По умолчанию используем 30% (как для 1h)
                        window_percentage = 0.30
                        self.logger.debug(f"Таймфрейм не определен, используем {window_percentage*100}% истории по умолчанию")
                    
                    # Вычисляем размер окна, минимум 20 периодов
                    window_size = max(20, int(len(df) * window_percentage))
                    self.logger.debug(f"Расчет адаптивных уровней с окном {window_size} периодов")
                    
                    # Используем quantile для ускорения расчетов вместо apply+percentile
                    df['rsi_80_percentile'] = df['rsi'].rolling(window=window_size).quantile(0.80)
                    df['rsi_20_percentile'] = df['rsi'].rolling(window=window_size).quantile(0.20)
                    
                    # Ограничиваем диапазон уровней для избежания слишком узких границ
                    df['rsi_80_percentile'] = np.maximum(df['rsi_80_percentile'], 65)  # Минимум 65
                    df['rsi_20_percentile'] = np.minimum(df['rsi_20_percentile'], 35)  # Максимум 35
                    
                    # Заполняем NaN значения для первых строк
                    df['rsi_80_percentile'] = df['rsi_80_percentile'].fillna(70)
                    df['rsi_20_percentile'] = df['rsi_20_percentile'].fillna(30)
                    
                    self.logger.debug(f"Средний верхний адаптивный уровень: {df['rsi_80_percentile'].mean()}")
                    self.logger.debug(f"Средний нижний адаптивный уровень: {df['rsi_20_percentile'].mean()}")
                
                # Заполняем NaN на 50 - нейтральное значение для RSI
                df['rsi'] = df['rsi'].fillna(50)
                
                # Добавляем столбец для скорости изменения RSI (для определения импульса)
                df['rsi_change'] = df['rsi'].diff(3)
                
                self.logger.info("Расчет RSI по методу Уайлдера успешно завершен")
                
                # Логируем базовую статистику результатов
                self.logger.info(f"Статистика RSI: min={df['rsi'].min():.2f}, max={df['rsi'].max():.2f}, mean={df['rsi'].mean():.2f}")
                self.logger.info(f"Текущее значение RSI: {df['rsi'].iloc[-1]:.2f}")
                self.logger.debug(f"Расчет RSI занял {time.time() - start_time:.4f} секунд")
            else:
                self.logger.warning(f"Недостаточно данных для расчета RSI (требуется минимум {self.period} периодов)")
                # Если данных недостаточно, заполняем нейтральным значением
                df['rsi'] = 50

            return df
        except Exception as e:
            self.logger.error(f"Ошибка при расчете RSI: {e}")
            self.logger.debug(traceback.format_exc())
            # В случае ошибки добавляем пустой столбец RSI
            df['rsi'] = [None] * len(df)
            return df
    
    def generate_signals(self, df: pd.DataFrame) -> List[Dict]:
        """
        Генерирует торговые сигналы на основе RSI.
        
        Сигналы:
        - Перепроданность (RSI < oversold): возможная покупка
        - Перекупленность (RSI > overbought): возможная продажа
        - Пересечение нейтрального уровня 50
        - Дивергенции RSI и цены
        
        Args:
            df: DataFrame с данными и рассчитанным RSI
            
        Returns:
            Список отфильтрованных и приоритезированных сигналов
        """
        signals = []
        
        if 'rsi' not in df.columns:
            self.logger.warning("Столбец 'rsi' не найден в DataFrame. Сигналы не сгенерированы.")
            return signals
        
        try:
            # Определяем уровни перекупленности/перепроданности
            if self.adaptive_levels and 'rsi_80_percentile' in df.columns:
                overbought_level = df['rsi_80_percentile']
                oversold_level = df['rsi_20_percentile']
            else:
                overbought_level = pd.Series([self.overbought] * len(df), index=df.index)
                oversold_level = pd.Series([self.oversold] * len(df), index=df.index)
            
            # 1. Сигналы перепроданности
            oversold_condition = df['rsi'] < oversold_level
            if oversold_condition.any():
                # Находим точки пересечения уровня перепроданности снизу вверх
                crosses_up = (df['rsi'].shift(1) < oversold_level.shift(1)) & (df['rsi'] >= oversold_level)
                
                for idx in df.index[crosses_up]:
                    signals.append({
                        'indicator': 'RSI',
                        'signal_type': SignalType.BUY.value,
                        'timestamp': str(idx) if not isinstance(idx, str) else idx,
                        'price': df.loc[idx, 'close'],
                        'strength': min(1.0, 1 - (df.loc[idx, 'rsi'] / float(oversold_level.loc[idx]))),
                        'description': f'RSI пересек уровень перепроданности {float(oversold_level.loc[idx]):.1f} снизу вверх',
                        'value': float(df.loc[idx, 'rsi'])
                    })
            
            # 2. Сигналы перекупленности
            overbought_condition = df['rsi'] > overbought_level
            if overbought_condition.any():
                # Находим точки пересечения уровня перекупленности сверху вниз
                crosses_down = (df['rsi'].shift(1) > overbought_level.shift(1)) & (df['rsi'] <= overbought_level)
                
                for idx in df.index[crosses_down]:
                    signals.append({
                        'indicator': 'RSI',
                        'signal_type': SignalType.SELL.value,
                        'timestamp': str(idx) if not isinstance(idx, str) else idx,
                        'price': df.loc[idx, 'close'],
                        'strength': min(1.0, (df.loc[idx, 'rsi'] - float(overbought_level.loc[idx])) / (100 - float(overbought_level.loc[idx]))),
                        'description': f'RSI пересек уровень перекупленности {float(overbought_level.loc[idx]):.1f} сверху вниз',
                        'value': float(df.loc[idx, 'rsi'])
                    })
            
            # 3. Сигналы пересечения центральной линии (50)
            crosses_above_50 = (df['rsi'].shift(1) < 50) & (df['rsi'] > 50)
            crosses_below_50 = (df['rsi'].shift(1) > 50) & (df['rsi'] < 50)
            
            for idx in df.index[crosses_above_50]:
                signals.append({
                    'indicator': 'RSI',
                    'signal_type': SignalType.BUY.value,
                    'timestamp': str(idx) if not isinstance(idx, str) else idx,
                    'price': df.loc[idx, 'close'],
                    'strength': 0.5 + min(0.5, abs(df.loc[idx, 'rsi'] - 50) / 50),
                    'description': 'RSI пересек центральный уровень 50 снизу вверх',
                    'value': float(df.loc[idx, 'rsi'])
                })
                
            for idx in df.index[crosses_below_50]:
                signals.append({
                    'indicator': 'RSI',
                    'signal_type': SignalType.SELL.value,
                    'timestamp': str(idx) if not isinstance(idx, str) else idx,
                    'price': df.loc[idx, 'close'],
                    'strength': 0.5 + min(0.5, abs(df.loc[idx, 'rsi'] - 50) / 50),
                    'description': 'RSI пересек центральный уровень 50 сверху вниз',
                    'value': float(df.loc[idx, 'rsi'])
                })
            
            # 4. Поиск дивергенций (с полноценной проверкой трендов)
            self._detect_divergences(df, signals)
            
            # 5. Фильтрация сигналов для получения более качественных результатов
            if len(signals) > 0:
                self.logger.debug(f"Сгенерировано {len(signals)} сигналов, применяем фильтрацию")
                
                # Фокусируемся только на недавних сигналах (последние 5 свечей)
                recent_periods = 5
                if len(df) >= recent_periods:
                    recent_indices = set(df.index[-recent_periods:].astype(str))
                    recent_signals = [s for s in signals if s['timestamp'] in recent_indices]
                    
                    # Если есть недавние сигналы, используем только их
                    if recent_signals:
                        self.logger.debug(f"Найдено {len(recent_signals)} недавних сигналов из общего числа {len(signals)}")
                        
                        # Сортируем по силе сигнала
                        recent_signals.sort(key=lambda x: x.get('strength', 0), reverse=True)
                        
                        # Оставляем только топ-3 сигнала, если их больше
                        filtered_signals = recent_signals[:min(3, len(recent_signals))]
                        
                        self.logger.debug(f"После фильтрации осталось {len(filtered_signals)} сигналов")
                        return filtered_signals
                
                # Если недавних сигналов нет, возвращаем самый последний сильный сигнал
                # Сортируем по времени (от новых к старым)
                sorted_by_time = sorted(signals, key=lambda x: x['timestamp'], reverse=True)
                
                # Берем сильнейшие из последних 3 сигналов
                recent_subset = sorted_by_time[:min(3, len(sorted_by_time))]
                top_recent = sorted(recent_subset, key=lambda x: x.get('strength', 0), reverse=True)
                
                if top_recent:
                    self.logger.debug(f"Недавних сигналов нет, возвращаем последний сильный сигнал из {len(signals)} общих")
                    return [top_recent[0]]  # Возвращаем только самый сильный
            
        except Exception as e:
            self.logger.error(f"Ошибка при генерации сигналов RSI: {e}")
            self.logger.debug(traceback.format_exc())
        
        return signals  # Если функция дошла сюда, возвращаем все сигналы (для совместимости)
    
    def _detect_divergences(self, df: pd.DataFrame, signals: List[Dict]) -> None:
        """
        Обнаруживает дивергенции между RSI и ценой с использованием адаптивных параметров.
        
        Args:
            df: DataFrame с данными и рассчитанным RSI
            signals: Список для добавления сигналов о дивергенциях
        """
        try:
            # Для обнаружения дивергенций нужно достаточное количество данных
            if len(df) < 20:
                return
            
            # Используем адаптивный параметр order в зависимости от длины данных и таймфрейма
            adaptive_order = max(3, min(21, len(df) // 100))
            
            # Расширяем для недельных и дневных таймфреймов
            if hasattr(df, 'timeframe') and getattr(df, 'timeframe') in ['7d', '24h']:
                adaptive_order = max(adaptive_order, 3)
            
            self.logger.debug(f"Поиск дивергенций с адаптивным параметром order={adaptive_order}")
            
            # Находим локальные экстремумы цены и RSI
            price_highs = argrelextrema(df['close'].values, np.greater, order=adaptive_order)[0]
            price_lows = argrelextrema(df['close'].values, np.less, order=adaptive_order)[0]
            rsi_highs = argrelextrema(df['rsi'].values, np.greater, order=adaptive_order)[0]
            rsi_lows = argrelextrema(df['rsi'].values, np.less, order=adaptive_order)[0]
            
            # Рассчитываем волатильность для адаптивных порогов
            volatility = df['close'].pct_change().rolling(window=14).std().mean() * 100  # Средняя волатильность в %
            
            # Адаптивные пороги на основе волатильности
            price_threshold = max(0.005, min(0.02, volatility / 10))  # Минимум 0.5%, максимум 2%
            rsi_threshold = max(0.03, min(0.10, volatility / 5))     # Минимум 3%, максимум 10%
            
            self.logger.debug(f"Волатильность: {volatility:.2f}%, пороги дивергенции: цена={price_threshold:.4f}, RSI={rsi_threshold:.4f}")
            
            # Словарь для хранения уже обнаруженных дивергенций (чтобы избежать дублирования)
            detected_divergences = set()
            
            # Проверяем медвежьи дивергенции (цена растет, RSI падает)
            self._analyze_bearish_divergences(df, price_highs, detected_divergences, signals, price_threshold, rsi_threshold)
            
            # Проверяем бычьи дивергенции (цена падает, RSI растет)
            self._analyze_bullish_divergences(df, price_lows, detected_divergences, signals, price_threshold, rsi_threshold)
            
        except Exception as e:
            self.logger.error(f"Ошибка при поиске дивергенций RSI: {e}")
            self.logger.debug(traceback.format_exc())
            
    def _analyze_bearish_divergences(self, df, price_highs, detected_divergences, signals, price_threshold, rsi_threshold):
        """Анализирует медвежьи дивергенции и добавляет сигналы при их обнаружении."""
        
        # Проверяем только последние 10 экстремумов для эффективности
        for i in range(1, min(len(price_highs), 10)):
            if len(price_highs) <= i or price_highs[-i] >= len(df) - 5:
                continue
            
            ph_current = price_highs[-i]
            
            # Ищем предыдущий ценовой максимум
            prev_ph = None
            for j in range(i+1, min(len(price_highs), i+5)):
                if price_highs[-j] < ph_current - 3:  # минимальное расстояние между экстремумами
                    prev_ph = price_highs[-j]
                    break
            
            if prev_ph is None:
                continue
            
            # Проверяем, растет ли цена и падает ли RSI с учетом адаптивных порогов
            price_change_pct = (df['close'].iloc[ph_current] / df['close'].iloc[prev_ph]) - 1
            rsi_change_pct = (df['rsi'].iloc[ph_current] / df['rsi'].iloc[prev_ph]) - 1
            
            # Дивергенция: цена выше, RSI ниже (с учетом порогов)
            if (price_change_pct > price_threshold and rsi_change_pct < -rsi_threshold):
                
                # Создаем уникальный ключ для этой дивергенции
                div_key = f"bearish_{prev_ph}_{ph_current}"
                if div_key in detected_divergences:
                    continue
                    
                detected_divergences.add(div_key)
                
                # Определяем силу дивергенции по разнице в направлениях
                divergence_strength = min(0.95, abs(price_change_pct - rsi_change_pct) * 5)
                
                # Классифицируем дивергенцию
                if divergence_strength > 0.7:
                    div_class = "A"  # Сильная
                elif divergence_strength > 0.4:
                    div_class = "B"  # Средняя
                else:
                    div_class = "C"  # Слабая
                
                signals.append({
                    'indicator': 'RSI',
                    'signal_type': SignalType.SELL.value,
                    'timestamp': str(df.index[ph_current]) if not isinstance(df.index[ph_current], str) else df.index[ph_current],
                    'price': float(df['close'].iloc[ph_current]),
                    'strength': divergence_strength,
                    'description': f'Медвежья дивергенция класса {div_class}: цена +{price_change_pct*100:.1f}%, RSI {rsi_change_pct*100:.1f}%',
                    'value': float(df['rsi'].iloc[ph_current]),
                    'divergence': {
                        'type': 'bearish',
                        'class': div_class,
                        'price_change': float(price_change_pct),
                        'rsi_change': float(rsi_change_pct),
                        'start_index': int(prev_ph),
                        'end_index': int(ph_current)
                    }
                })

    def _analyze_bullish_divergences(self, df, price_lows, detected_divergences, signals, price_threshold, rsi_threshold):
        """Анализирует бычьи дивергенции и добавляет сигналы при их обнаружении."""
        
        # Проверяем только последние 10 экстремумов для эффективности
        for i in range(1, min(len(price_lows), 10)):
            if len(price_lows) <= i or price_lows[-i] >= len(df) - 5:
                continue
            
            pl_current = price_lows[-i]
            
            # Ищем предыдущий ценовой минимум
            prev_pl = None
            for j in range(i+1, min(len(price_lows), i+5)):
                if price_lows[-j] < pl_current - 3:  # минимальное расстояние между экстремумами
                    prev_pl = price_lows[-j]
                    break
            
            if prev_pl is None:
                continue
            
            # Проверяем, падает ли цена и растет ли RSI с учетом адаптивных порогов
            price_change_pct = (df['close'].iloc[pl_current] / df['close'].iloc[prev_pl]) - 1
            rsi_change_pct = (df['rsi'].iloc[pl_current] / df['rsi'].iloc[prev_pl]) - 1
            
            # Дивергенция: цена ниже, RSI выше (с учетом порогов)
            if (price_change_pct < -price_threshold and rsi_change_pct > rsi_threshold):
                
                # Создаем уникальный ключ для этой дивергенции
                div_key = f"bullish_{prev_pl}_{pl_current}"
                if div_key in detected_divergences:
                    continue
                    
                detected_divergences.add(div_key)
                
                # Определяем силу дивергенции по разнице в направлениях
                divergence_strength = min(0.95, abs(price_change_pct - rsi_change_pct) * 5)
                
                # Классифицируем дивергенцию
                if divergence_strength > 0.7:
                    div_class = "A"  # Сильная
                elif divergence_strength > 0.4:
                    div_class = "B"  # Средняя
                else:
                    div_class = "C"  # Слабая
                
                signals.append({
                    'indicator': 'RSI',
                    'signal_type': SignalType.BUY.value,
                    'timestamp': str(df.index[pl_current]) if not isinstance(df.index[pl_current], str) else df.index[pl_current],
                    'price': float(df['close'].iloc[pl_current]),
                    'strength': divergence_strength,
                    'description': f'Бычья дивергенция класса {div_class}: цена {price_change_pct*100:.1f}%, RSI +{rsi_change_pct*100:.1f}%',
                    'value': float(df['rsi'].iloc[pl_current]),
                    'divergence': {
                        'type': 'bullish',
                        'class': div_class,
                        'price_change': float(price_change_pct),
                        'rsi_change': float(rsi_change_pct),
                        'start_index': int(prev_pl),
                        'end_index': int(pl_current)
                    }
                })
    
    def analyze_context(self, df: pd.DataFrame, timeframe: str) -> IndicatorAnalysisResult:
        """
        Выполняет контекстуальный анализ RSI на основе всей истории.
        
        Контекстуальный анализ включает:
        - Текущее положение RSI в историческом контексте (перцентили)
        - Анализ эффективности сигналов RSI в разных фазах рынка
        - Определение трендов RSI и их сравнение с ценовыми трендами
        - Рекомендации на основе исторической эффективности
        
        Args:
            df: DataFrame с данными и рассчитанным RSI
            timeframe: Таймфрейм анализа ('5m', '15m', '1h', и т.д.)
            
        Returns:
            IndicatorAnalysisResult с результатами анализа
        """

        start_time = time.time()
        self.logger.info(f"Начало контекстуального анализа RSI для таймфрейма {timeframe}")

        try:
            if 'rsi' not in df.columns or len(df) < 30:
                self.logger.warning(f"Недостаточно данных для контекстуального анализа RSI ({timeframe})")
                return IndicatorAnalysisResult(
                    indicator_name="RSI",
                    current_value=None,
                    historical_context={},
                    market_phases={},
                    signals=[],
                    statistics={},
                    recommendation=SignalType.NEUTRAL.value,
                    confidence=0.0,
                    metadata={"error": "Недостаточно данных для анализа"}
                )
            
            # Получаем текущее значение RSI
            current_rsi = float(df['rsi'].iloc[-1])
            
            # 1. Исторический контекст RSI
            historical_context = self._analyze_historical_context(df)
            
            # 2. Анализ по фазам рынка
            market_phases = {
                "unknown": {
                    "segment_count": 1,
                    "avg_rsi": float(df['rsi'].mean()),
                    "buy_success_rate": None,
                    "sell_success_rate": None,
                    "signal_count": 0
                },
                "current_phase": MarketPhase.UNKNOWN.value
            }
            
            # 3. Генерация сигналов
            signals = self.generate_signals(df)
            
            # Фильтруем сигналы для последних N периодов
            recent_periods = min(20, len(df) // 10)  # ~10% от всей истории, но не более 20 периодов
            recent_signals = [s for s in signals if s['timestamp'] in df.index[-recent_periods:].astype(str)]
            
            # 4. Расчет статистики
            statistics = self._calculate_statistics(df)
            
            # 5. Формирование рекомендации
            recommendation, confidence = self._generate_recommendation(
                df, current_rsi, historical_context, market_phases, recent_signals
            )
            
            # Создаем результат анализа
            result = IndicatorAnalysisResult(
                indicator_name="RSI",
                current_value=current_rsi,
                historical_context=historical_context,
                market_phases=market_phases,
                signals=recent_signals,
                statistics=statistics,
                recommendation=recommendation,
                confidence=confidence,
                metadata={
                    "timeframe": timeframe,
                    "analysis_timestamp": datetime.now().isoformat(),
                    "symbol": getattr(df, 'symbol', 'unknown'),
                    "data_points": len(df),
                    "period": self.period
                }
            )
            
            self.logger.info(f"Завершение контекстуального анализа RSI. Результат: {recommendation}, уверенность: {confidence:.2f}")
            self.logger.debug(f"Контекстуальный анализ RSI занял {time.time() - start_time:.4f} секунд")

            return result
            
        except Exception as e:
            self.logger.error(f"Ошибка при контекстуальном анализе RSI: {e}")
            self.logger.debug(traceback.format_exc())
            
            # Возвращаем базовый результат с ошибкой
            return IndicatorAnalysisResult(
                indicator_name="RSI",
                current_value=None,
                historical_context={},
                market_phases={},
                signals=[],
                statistics={},
                recommendation=SignalType.NEUTRAL.value,
                confidence=0.0,
                metadata={"error": str(e)}
            )
    
    def _analyze_historical_context(self, df: pd.DataFrame) -> Dict:
        """
        Анализирует исторический контекст RSI с улучшенным расчетом метрик.
        
        Args:
            df: DataFrame с данными и рассчитанным RSI
            
        Returns:
            Dict: Словарь с расширенным историческим контекстом
        """
        rsi_values = df['rsi'].dropna()
        current_rsi = rsi_values.iloc[-1]
        
        # Рассчитываем перцентили для текущего значения RSI
        percentile = stats.percentileofscore(rsi_values, current_rsi)
        
        # Квартили RSI за всю историю
        quartiles = [0, 25, 50, 75, 100]
        percentiles = [np.percentile(rsi_values, q) for q in quartiles]
        
        # Определяем, в каком историческом квартиле находится текущее значение
        current_quartile = None
        for i in range(1, len(quartiles)):
            if percentiles[i-1] <= current_rsi <= percentiles[i]:
                current_quartile = i
                break
        
        # Анализируем исторические зоны перекупленности/перепроданности
        historical_overbought = rsi_values > self.overbought
        historical_oversold = rsi_values < self.oversold
        
        # Рассчитываем, сколько % времени RSI был в зонах перекупленности/перепроданности
        pct_overbought = historical_overbought.mean() * 100
        pct_oversold = historical_oversold.mean() * 100
        
        # Определяем исторические экстремумы
        rsi_max = rsi_values.max()
        rsi_min = rsi_values.min()
        
        # Определяем, как долго обычно RSI находится в зонах перекупленности/перепроданности
        if historical_overbought.any():
            overbought_durations = self._calculate_zone_durations(historical_overbought)
            avg_overbought_duration = np.mean(overbought_durations)
            max_overbought_duration = np.max(overbought_durations)
        else:
            avg_overbought_duration = 0
            max_overbought_duration = 0
            
        if historical_oversold.any():
            oversold_durations = self._calculate_zone_durations(historical_oversold)
            avg_oversold_duration = np.mean(oversold_durations)
            max_oversold_duration = np.max(oversold_durations)
        else:
            avg_oversold_duration = 0
            max_oversold_duration = 0
        
        # Анализируем, как быстро RSI обычно возвращается из зон экстремумов
        return_from_overbought = self._analyze_returns_from_zones(df, historical_overbought)
        return_from_oversold = self._analyze_returns_from_zones(df, historical_oversold)
        
        # Определяем адаптивные уровни на основе исторического распределения
        adaptive_overbought = np.percentile(rsi_values, 80)  # 80-й перцентиль
        adaptive_oversold = np.percentile(rsi_values, 20)    # 20-й перцентиль

        # Обеспечиваем минимальный разброс между уровнями
        adaptive_overbought, adaptive_oversold = self._ensure_min_adaptive_range(adaptive_overbought, adaptive_oversold)    
        
        # Определяем таймфрейм и количество периодов в дне
        timeframe = getattr(df, 'timeframe', '1h')
        periods_per_day = {
            '5m': 288,
            '15m': 96,
            '30m': 48,
            '1h': 24,
            '4h': 6,
            '24h': 1,
            '7d': 1/7
        }.get(timeframe, 24)  # По умолчанию 1h
        
        # Рассчитываем тренд RSI за последние N периодов
        recent_window = min(20, len(rsi_values) // 4)
        recent_rsi_trend = None
        if recent_window > 0:
            recent_values = rsi_values.iloc[-recent_window:]
            if len(recent_values) > 1:
                slope, intercept, r_value, p_value, std_err = stats.linregress(range(len(recent_values)), recent_values)
                recent_rsi_trend = {
                    "slope": float(slope),
                    "r_squared": float(r_value ** 2),
                    "p_value": float(p_value),
                    "trend_type": "восходящий" if slope > 0 else "нисходящий"
                }
        
        # Возвращаем расширенный контекст
        return {
            "current_percentile": percentile,
            "historical_max": float(rsi_max),
            "historical_min": float(rsi_min),
            "avg_value": float(rsi_values.mean()),
            "median_value": float(rsi_values.median()),
            "std_dev": float(rsi_values.std()),
            "percent_time_overbought": float(pct_overbought),
            "percent_time_oversold": float(pct_oversold),
            "avg_overbought_duration_periods": float(avg_overbought_duration),
            "max_overbought_duration_periods": float(max_overbought_duration),
            "avg_oversold_duration_periods": float(avg_oversold_duration),
            "max_oversold_duration_periods": float(max_oversold_duration),
            "avg_periods_to_return_from_overbought": float(return_from_overbought.get('avg_periods', 0)),
            "avg_periods_to_return_from_oversold": float(return_from_oversold.get('avg_periods', 0)),
            "adaptive_overbought_level": float(adaptive_overbought),
            "adaptive_oversold_level": float(adaptive_oversold),
            "historical_quartiles": {
                "q0": float(percentiles[0]),  # Минимум
                "q25": float(percentiles[1]),  # 25-й процентиль
                "q50": float(percentiles[2]),  # Медиана
                "q75": float(percentiles[3]),  # 75-й процентиль
                "q100": float(percentiles[4])  # Максимум
            },
            "current_quartile": current_quartile,  # В каком квартиле находится текущее значение
            "days_of_history": len(df) / periods_per_day,  # Примерное количество дней истории
            "recent_rsi_trend": recent_rsi_trend  # Тренд RSI за последние периоды
        }
    
    def _calculate_zone_durations(self, zone_mask: pd.Series) -> List[int]:
        """
        Рассчитывает продолжительность пребывания RSI в определенной зоне.
        
        Args:
            zone_mask: Маска с True для периодов, когда RSI находится в зоне
            
        Returns:
            List[int]: Список продолжительностей пребывания в зоне (в периодах)
        """
        # Определяем границы зон (True/False)
        zone_starts = (zone_mask != zone_mask.shift(1)) & zone_mask
        zone_ends = (zone_mask != zone_mask.shift(-1)) & zone_mask
        
        # Получаем индексы начала и конца каждой зоны
        start_indices = zone_starts[zone_starts].index.tolist()
        end_indices = zone_ends[zone_ends].index.tolist()
        
        # Если количество начал и концов не совпадает, корректируем
        if len(start_indices) > len(end_indices):
            start_indices = start_indices[:len(end_indices)]
        elif len(end_indices) > len(start_indices):
            end_indices = end_indices[:len(start_indices)]
        
        # Рассчитываем продолжительность каждой зоны
        durations = []
        for i in range(len(start_indices)):
            start_idx = zone_mask.index.get_loc(start_indices[i])
            end_idx = zone_mask.index.get_loc(end_indices[i])
            duration = end_idx - start_idx + 1
            durations.append(duration)
        
        return durations if durations else [0]
    
    def _analyze_returns_from_zones(self, df: pd.DataFrame, zone_mask: pd.Series) -> Dict:
        """
        Анализирует, как быстро RSI возвращается из зоны перекупленности/перепроданности.
        
        Args:
            df: DataFrame с данными и рассчитанным RSI
            zone_mask: Маска с True для периодов, когда RSI находится в зоне
            
        Returns:
            Dict: Словарь с результатами анализа
        """
        if not zone_mask.any():
            return {"avg_periods": 0, "max_periods": 0, "min_periods": 0}
        
        # Находим выходы из зоны
        zone_exits = (zone_mask != zone_mask.shift(-1)) & zone_mask
        
        periods_to_return = []
        
        for idx in zone_exits[zone_exits].index:
            idx_pos = df.index.get_loc(idx)
            
            # Пропускаем, если это последний индекс
            if idx_pos >= len(df) - 1:
                continue
            
            # Определяем, в какую зону вышли (из перекупленности вниз или из перепроданности вверх)
            if df.loc[idx, 'rsi'] > 50:  # Выход из зоны перекупленности
                target = 50  # Цель - пересечение 50
            else:  # Выход из зоны перепроданности
                target = 50  # Цель - пересечение 50
            
            # Ищем, когда RSI достигнет целевого значения
            for i in range(idx_pos + 1, min(idx_pos + 50, len(df))):
                cross_down = df.iloc[i-1]['rsi'] > target and df.iloc[i]['rsi'] <= target
                cross_up = df.iloc[i-1]['rsi'] < target and df.iloc[i]['rsi'] >= target
                
                if cross_down or cross_up:
                    periods_to_return.append(i - idx_pos)
                    break
        
        if periods_to_return:
            return {
                "avg_periods": np.mean(periods_to_return),
                "max_periods": np.max(periods_to_return),
                "min_periods": np.min(periods_to_return)
            }
        else:
            return {"avg_periods": 0, "max_periods": 0, "min_periods": 0}
    
    def _analyze_market_phases(self, df: pd.DataFrame) -> Dict:
        """
        Анализирует поведение RSI в различных фазах рынка.
        
        Args:
            df: DataFrame с данными и рассчитанным RSI
            
        Returns:
            Dict: Словарь с результатами анализа по фазам рынка
        """
        results = {}
        
        # Для полноценного анализа фаз рынка нужно достаточное количество данных
        if len(df) < 50:
            return {"error": "Недостаточно данных для анализа фаз рынка"}
        
        # Разбиваем историю на сегменты (примерно по 20-30 баров)
        segment_size = min(30, max(20, len(df) // 10))
        segments = [df.iloc[i:i+segment_size] for i in range(0, len(df), segment_size)]
        
        # Анализируем каждый сегмент
        phase_data = defaultdict(list)
        
        for segment in segments:
            if len(segment) < 10:  # Пропускаем слишком короткие сегменты
                continue
                
            # Определяем фазу рынка для этого сегмента
            phase = self.detect_market_phase(segment)
            
            # Рассчитываем средние значения RSI для этой фазы
            avg_rsi = segment['rsi'].mean()
            
            # Рассчитываем эффективность сигналов RSI в этой фазе
            signals = self.generate_signals(segment)
            
            # Оцениваем результаты сигналов (просто оценка, без реального бэктеста)
            buy_success, sell_success = self._evaluate_signals_simple(segment, signals)
            
            # Добавляем данные для этой фазы
            phase_data[phase.value].append({
                "avg_rsi": float(avg_rsi),
                "signal_count": len(signals),
                "buy_success_rate": buy_success,
                "sell_success_rate": sell_success
            })
        
        # Агрегируем данные по фазам
        for phase, phase_segments in phase_data.items():
            if not phase_segments:
                continue
                
            avg_rsi_values = [seg["avg_rsi"] for seg in phase_segments]
            buy_success_rates = [seg["buy_success_rate"] for seg in phase_segments if seg["buy_success_rate"] is not None]
            sell_success_rates = [seg["sell_success_rate"] for seg in phase_segments if seg["sell_success_rate"] is not None]
            
            results[phase] = {
                "segment_count": len(phase_segments),
                "avg_rsi": np.mean(avg_rsi_values) if avg_rsi_values else None,
                "buy_success_rate": np.mean(buy_success_rates) if buy_success_rates else None,
                "sell_success_rate": np.mean(sell_success_rates) if sell_success_rates else None,
                "signal_count": sum(seg["signal_count"] for seg in phase_segments)
            }
        
        # Определяем текущую фазу рынка
        current_segment = df.iloc[-min(30, len(df)):]
        current_phase = self.detect_market_phase(current_segment).value
        
        results["current_phase"] = current_phase
        
        return results
    
    def _evaluate_signals_simple(self, df: pd.DataFrame, signals: List[Dict]) -> Tuple[Optional[float], Optional[float]]:
        """
        Простая оценка эффективности сигналов RSI без проведения полного бэктеста.
        
        Args:
            df: DataFrame с данными
            signals: Список сигналов
            
        Returns:
            Tuple[Optional[float], Optional[float]]: (процент успешных покупок, процент успешных продаж)
        """
        if not signals:
            return None, None
        
        buy_signals = [s for s in signals if s['signal_type'] == SignalType.BUY.value]
        sell_signals = [s for s in signals if s['signal_type'] == SignalType.SELL.value]
        
        if not buy_signals and not sell_signals:
            return None, None
        
        # Оцениваем успешность покупок (если цена выросла в течение N периодов после сигнала)
        buy_success_count = 0
        for signal in buy_signals:
            try:
                signal_idx = df.index.get_loc(pd.Timestamp(signal['timestamp']) if not isinstance(signal['timestamp'], str) else signal['timestamp'])
                
                # Пропускаем сигналы на границе данных
                if signal_idx >= len(df) - 5:
                    continue
                
                # Проверяем, выросла ли цена на 2% или более в течение 5 баров после сигнала
                entry_price = signal['price']
                future_prices = df['close'].iloc[signal_idx+1:min(signal_idx+6, len(df))]
                
                if len(future_prices) > 0:
                    max_price = future_prices.max()
                    if max_price / entry_price >= 1.02:  # 2% рост
                        buy_success_count += 1
            except Exception:
                continue
        
        # Оцениваем успешность продаж (если цена упала в течение N периодов после сигнала)
        sell_success_count = 0
        for signal in sell_signals:
            try:
                signal_idx = df.index.get_loc(pd.Timestamp(signal['timestamp']) if not isinstance(signal['timestamp'], str) else signal['timestamp'])
                
                # Пропускаем сигналы на границе данных
                if signal_idx >= len(df) - 5:
                    continue
                
                # Проверяем, упала ли цена на 2% или более в течение 5 баров после сигнала
                entry_price = signal['price']
                future_prices = df['close'].iloc[signal_idx+1:min(signal_idx+6, len(df))]
                
                if len(future_prices) > 0:
                    min_price = future_prices.min()
                    if entry_price / min_price >= 1.02:  # 2% падение
                        sell_success_count += 1
            except Exception:
                continue
        
        # Рассчитываем процент успешных сигналов
        buy_success_rate = buy_success_count / len(buy_signals) if buy_signals else None
        sell_success_rate = sell_success_count / len(sell_signals) if sell_signals else None
        
        return buy_success_rate, sell_success_rate
    
    def _calculate_statistics(self, df: pd.DataFrame) -> Dict:
        """
        Рассчитывает статистические метрики для RSI.
        
        Args:
            df: DataFrame с данными и рассчитанным RSI
            
        Returns:
            Dict: Словарь со статистикой
        """
        rsi_values = df['rsi'].dropna()
        
        if len(rsi_values) < 10:
            return {"error": "Недостаточно данных для расчета статистики"}
        
        # Рассчитываем базовые статистики
        stats_data = {
            "mean": float(rsi_values.mean()),
            "median": float(rsi_values.median()),
            "std_dev": float(rsi_values.std()),
            "min": float(rsi_values.min()),
            "max": float(rsi_values.max()),
            "skew": float(rsi_values.skew()),  # Асимметрия распределения
            "kurtosis": float(rsi_values.kurtosis()),  # Эксцесс распределения
            "histogram_bins": np.histogram(rsi_values, bins=10)[0].tolist()
        }
        
        # Рассчитываем автокорреляцию (для оценки цикличности)
        try:
            autocorr = pd.Series(rsi_values).autocorr(lag=7)  # Автокорреляция с лагом 7
            stats_data["autocorrelation_lag7"] = float(autocorr) if not np.isnan(autocorr) else 0
        except Exception:
            stats_data["autocorrelation_lag7"] = 0
        
        # Оцениваем тренд RSI
        recent_window = min(20, len(rsi_values) // 4)
        if recent_window > 0:
            recent_values = rsi_values.iloc[-recent_window:]
            if len(recent_values) > 1:
                slope, intercept, r_value, p_value, std_err = stats.linregress(range(len(recent_values)), recent_values)
                stats_data["recent_trend"] = {
                    "slope": float(slope),
                    "r_squared": float(r_value ** 2),
                    "p_value": float(p_value)
                }
        
        return stats_data
    
    def _generate_recommendation(self, df: pd.DataFrame, current_rsi: float, 
                            historical_context: Dict, market_phases: Dict, 
                            recent_signals: List[Dict]) -> Tuple[str, float]:
        """
        Генерирует рекомендацию на основе гибридного подхода, сочетающего квартильный анализ
        с распознаванием конкретных технических паттернов.
        
        Args:
            df: DataFrame с данными
            current_rsi: Текущее значение RSI
            historical_context: Исторический контекст
            market_phases: Анализ по фазам рынка
            recent_signals: Недавние сигналы
            
        Returns:
            Tuple[str, float]: (рекомендация, уверенность)
        """
        self.logger.debug(f"Генерация рекомендации с гибридным подходом, RSI={current_rsi:.2f}")
        
        # 1. Получаем квартильную информацию
        quartile = self._get_quartile(df, current_rsi)
        
        # 2. Определяем направление тренда RSI
        trend_direction = "flat"
        trend_strength = 0
        
        # Проверяем краткосрочный тренд (последние 5 свечей вместо 3)
        if len(df) >= 5:
            # Считаем, сколько из последних 5 свечей показывают рост/падение
            rising_count = sum(1 for i in range(1, 5) if df['rsi'].iloc[-i] > df['rsi'].iloc[-i-1])
            
            # Если большинство (>= 3) свечей показывают рост - тренд растущий
            if rising_count >= 3:
                short_trend = "rising"
            # Если большинство (>= 3) свечей показывают падение - тренд падающий
            elif rising_count <= 2:
                short_trend = "falling"
            else:
                short_trend = "flat"
            
            # Проверяем долгосрочный тренд по статистике
            long_trend = "flat"
            if 'recent_rsi_trend' in historical_context:
                trend = historical_context['recent_rsi_trend']
                slope = trend.get('slope', 0)
                r_squared = trend.get('r_squared', 0)
                
                if r_squared > 0.3:  # Статистически значимый тренд
                    trend_strength = min(1.0, r_squared + abs(slope) / 2)
                    if slope > 0.3:
                        long_trend = "rising"
                    elif slope < -0.3:
                        long_trend = "falling"
            
            # Приоритет отдаем короткому тренду при сильном противоречии
            # В остальных случаях используем долгосрочный тренд
            if short_trend != "flat" and short_trend != long_trend:
                trend_direction = short_trend
                self.logger.debug(f"Используем краткосрочный тренд: {short_trend} (противоречит долгосрочному: {long_trend})")
            else:
                trend_direction = long_trend if long_trend != "flat" else short_trend
                self.logger.debug(f"Используем тренд: {trend_direction} (согласованный)")
        
        # 3. Получаем адаптивные уровни с обеспечением минимального разброса
        adaptive_overbought = historical_context.get("adaptive_overbought_level", self.overbought)
        adaptive_oversold = historical_context.get("adaptive_oversold_level", self.oversold)
        
        # Обеспечиваем минимальный разброс между уровнями
        min_range = 20
        if adaptive_overbought - adaptive_oversold < min_range:
            mid_point = (adaptive_overbought + adaptive_oversold) / 2
            adaptive_overbought = mid_point + min_range / 2
            adaptive_oversold = mid_point - min_range / 2
            self.logger.debug(f"Расширены адаптивные уровни: {adaptive_oversold:.2f}/{adaptive_overbought:.2f} (мин. разброс {min_range})")
        
        # 4. Проверяем конкретные технические паттерны
        
        # Паттерн разворота от перепроданности (с подтверждением 5 свечей)
        oversold_reversal = False
        if current_rsi <= adaptive_oversold + 3:
            if len(df) >= 5:
                # Проверяем, что RSI снижался, а затем начал расти
                if (df['rsi'].iloc[-5] > df['rsi'].iloc[-4] > df['rsi'].iloc[-3] and 
                    df['rsi'].iloc[-3] < df['rsi'].iloc[-2] < df['rsi'].iloc[-1]):
                    oversold_reversal = True
                    self.logger.debug("Обнаружен паттерн разворота от перепроданности")
        
        # Паттерн пересечения 50 и продолжения тренда
        trend_continuation = False
        if 49 < current_rsi < 60 and trend_direction == "rising":
            # Поиск недавнего пересечения 50 снизу вверх
            for i in range(1, min(10, len(df))):
                if df['rsi'].iloc[-i-1] < 50 and df['rsi'].iloc[-i] >= 50:
                    trend_continuation = True
                    self.logger.debug(f"Обнаружен паттерн продолжения тренда (пересечение 50 на {i} свечей назад)")
                    break
        
        # 5. Формируем базовую рекомендацию по квартилям
        
        if quartile == 1:  # Нижний квартиль
            if trend_direction == "rising":
                base_rec = SignalType.BUY.value
                base_conf = 0.7 + 0.15 * trend_strength
                desc = "RSI в нижнем квартиле с восходящим трендом"
            elif trend_direction == "falling":
                base_rec = SignalType.SELL.value
                base_conf = 0.6 + 0.1 * trend_strength
                desc = "RSI в нижнем квартиле с нисходящим трендом"
            else:
                base_rec = SignalType.NEUTRAL.value
                base_conf = 0.5
                desc = "RSI в нижнем квартиле без явного тренда"
        
        elif quartile == 2:  # Второй квартиль
            if trend_direction == "rising" and current_rsi > 45:
                base_rec = SignalType.BUY.value
                base_conf = 0.6 + 0.15 * trend_strength
                desc = "RSI во втором квартиле с восходящим трендом выше 45"
            elif trend_direction == "falling" and current_rsi < 45:
                base_rec = SignalType.SELL.value
                base_conf = 0.6 + 0.15 * trend_strength
                desc = "RSI во втором квартиле с нисходящим трендом ниже 45"
            else:
                base_rec = SignalType.NEUTRAL.value
                base_conf = 0.5
                desc = "RSI во втором квартиле без сильных сигналов"
        
        elif quartile == 3:  # Третий квартиль
            if trend_direction == "rising":
                base_rec = SignalType.BUY.value
                base_conf = 0.6 + 0.1 * trend_strength
                desc = "RSI в третьем квартиле с восходящим трендом"
            elif trend_direction == "falling" and current_rsi < 55:
                base_rec = SignalType.SELL.value
                base_conf = 0.65 + 0.15 * trend_strength
                desc = "RSI в третьем квартиле с нисходящим трендом ниже 55"
            else:
                base_rec = SignalType.NEUTRAL.value
                base_conf = 0.5
                desc = "RSI в третьем квартиле без сильных сигналов"
        
        elif quartile == 4:  # Верхний квартиль
            if trend_direction == "falling":
                base_rec = SignalType.SELL.value
                base_conf = 0.7 + 0.15 * trend_strength
                desc = "RSI в верхнем квартиле с нисходящим трендом"
            elif trend_direction == "rising":
                base_rec = SignalType.NEUTRAL.value
                base_conf = 0.5
                desc = "RSI в верхнем квартиле с восходящим трендом (возможная перекупленность)"
            else:
                base_rec = SignalType.NEUTRAL.value
                base_conf = 0.5
                desc = "RSI в верхнем квартиле без явного тренда"
        
        # 6. Усиливаем сигнал, если есть конкретные паттерны
        
        final_rec = base_rec
        final_conf = base_conf
        
        if oversold_reversal and (base_rec == SignalType.BUY.value or base_rec == SignalType.NEUTRAL.value):
            final_rec = SignalType.BUY.value
            final_conf = max(base_conf, 0.75 + min(0.2, (adaptive_oversold - current_rsi + 3) / 15))
            desc = "Разворот RSI от перепроданности с подтверждением 5 свечей"
            
        elif trend_continuation and (base_rec == SignalType.BUY.value or base_rec == SignalType.NEUTRAL.value):
            final_rec = SignalType.BUY.value
            final_conf = max(base_conf, 0.7 + min(0.2, (current_rsi - 50) / 25))
            desc = "Продолжение восходящего тренда RSI после пересечения уровня 50"
            
        elif current_rsi > adaptive_overbought and trend_direction == "falling":
            final_rec = SignalType.SELL.value
            final_conf = max(base_conf, 0.75 + min(0.2, (current_rsi - adaptive_overbought) / 15))
            desc = "RSI выше уровня перекупленности с нисходящим трендом"
        
        # 7. Учитываем недавние сигналы для уточнения рекомендации
        
        if recent_signals:
            # Группируем сигналы по типу
            recent_buy_signals = [s for s in recent_signals if s['signal_type'] == SignalType.BUY.value]
            recent_sell_signals = [s for s in recent_signals if s['signal_type'] == SignalType.SELL.value]
            
            # Если есть сигналы того же типа, что и наша рекомендация, усиливаем уверенность
            if final_rec == SignalType.BUY.value and recent_buy_signals:
                strongest_signal = max(recent_buy_signals, key=lambda x: x.get('strength', 0))
                final_conf = min(0.95, final_conf + 0.05 * strongest_signal.get('strength', 0))
                
            elif final_rec == SignalType.SELL.value and recent_sell_signals:
                strongest_signal = max(recent_sell_signals, key=lambda x: x.get('strength', 0))
                final_conf = min(0.95, final_conf + 0.05 * strongest_signal.get('strength', 0))
        
        # 8. Ограничиваем уверенность в пределах [0.5, 0.95]
        final_conf = max(0.5, min(0.95, final_conf))
        
        self.logger.info(f"Финальная рекомендация: {final_rec} с уверенностью {final_conf:.2f} на основе: {desc}")
        
        return final_rec, final_conf


# ============================================================
# УПРАВЛЯЮЩИЙ КЛАСС INDICATOR ENGINE
# ============================================================

class IndicatorEngine:
    """
    Основной класс для управления анализом индикаторов.
    
    Координирует работу всех компонентов: загрузку данных, расчет индикаторов,
    анализ исторических данных, и формирование результатов.
    
    Функционирует как конструктор, позволяющий добавлять новые индикаторы без
    переписывания кода.
    """
    
    def __init__(self, config: Dict):
        """
        Инициализирует движок индикаторов.
        
        Args:
            config: Словарь с конфигурацией движка
        """
        self.config = config
        self.logger = get_clean_logger("indicator_engine")
        self.logger.info("Инициализация IndicatorEngine")
        
        # Инициализация утилит и хелперов
        self.data_cache = DataCache(
            use_redis=config.get('use_redis', False),
            redis_config=config.get('redis_config')
        )
        
        self.data_storage = DataStorage(
            base_path=config['paths'].get('BRAIN_DATA', 'data/brain/'),
            use_parquet=config.get('use_parquet', False)
        )
        
        self.parallel_processor = ParallelProcessor(
            max_workers=config['processing'].get('max_workers', 8),
            chunk_size=config['processing'].get('batch_size', 10)
        )
        
        # Инициализация реестра индикаторов
        self.indicator_registry = IndicatorRegistry()
        
        # Регистрация индикаторов
        self._register_indicators()
        
        self.logger.info(f"IndicatorEngine инициализирован с {len(self.indicator_registry.get_all())} индикаторами")
    
    def _register_indicators(self) -> None:
        """
        Регистрирует индикаторы в реестре согласно конфигурации.
        
        Это место, где добавляются новые индикаторы при расширении системы.
        """
        # Регистрация RSI
        rsi_enabled = self.config.get('indicators', {}).get('phase1', {}).get('momentum', {}).get('RSI', {}).get('enabled', True)
        if rsi_enabled:
            rsi_params = self.config.get('indicators', {}).get('phase1', {}).get('momentum', {}).get('RSI', {})
            self.indicator_registry.register(RSI, "RSI", params=rsi_params)
            self.logger.info("RSI индикатор зарегистрирован")
        
        # Здесь можно добавить другие индикаторы по мере их разработки
        # Например:
        # macd_enabled = self.config.get('indicators', {}).get('phase1', {}).get('trend', {}).get('MACD', {}).get('enabled', True)
        # if macd_enabled:
        #     macd_params = self.config.get('indicators', {}).get('phase1', {}).get('trend', {}).get('MACD', {})
        #     self.indicator_registry.register(MACD, "MACD", params=macd_params)
    
    def register_indicator(self, indicator_class: type, name: str = None, params: Dict = None) -> None:
        """
        Регистрирует новый индикатор в системе (внешний интерфейс).
        
        Args:
            indicator_class: Класс индикатора
            name: Имя для регистрации (если None, используется имя класса)
            params: Параметры для инициализации индикатора
        """
        self.indicator_registry.register(indicator_class, name, params)
    
    def load_data(self, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """
        Загружает данные для указанного символа и таймфрейма.
        
        Args:
            symbol: Символ монеты
            timeframe: Таймфрейм
            
        Returns:
            DataFrame с данными или None, если данные не найдены
        """

        start_time = time.time()

        try:
            # Базовая директория с данными
            base_path = os.path.join(self.config['paths'].get('DATA_ENGINE', {}).get('current', 'data/brain/data_engine/current'), timeframe)
            
            # Попробуем разные варианты имен файлов
            json_paths = [
                os.path.join(base_path, f"{symbol}_{timeframe}.json"),
                os.path.join(base_path, f"{symbol}.json"),
                os.path.join(self.config['paths'].get('DATA_ENGINE', {}).get('current', 'data/brain/data_engine/current'), f"{symbol}_{timeframe}.json"),
                os.path.join(self.config['paths'].get('DATA_ENGINE', {}).get('current', 'data/brain/data_engine/current'), f"{symbol}.json")
            ]
            
            # Для отладки выведем все проверяемые пути
            self.logger.debug(f"Ищем данные для {symbol} ({timeframe}) в путях: {json_paths}")
            
            # Ищем первый существующий файл
            json_path = None
            for path in json_paths:
                if os.path.exists(path):
                    json_path = path
                    self.logger.debug(f"Найден файл данных: {path}")
                    break
            
            if not json_path:
                self.logger.warning(f"Файлы с данными не найдены для {symbol} ({timeframe})")
                return None
            
            # Загружаем JSON данные
            self.logger.info(f"Загружаем данные из JSON: {json_path}")
            with open(json_path, 'r') as f:
                data = json.load(f)
            
            # Вывод первых нескольких элементов для отладки
            self.logger.debug(f"Первые 3 элемента JSON: {json.dumps(data[:3] if len(data) > 3 else data, indent=2)}")
            
            # Преобразуем специфический формат JSON в DataFrame
            rows = []
            for candle in data:
                # Проверяем наличие необходимых ключей
                if 'quote' not in candle or 'USD' not in candle['quote']:
                    continue
                    
                # Получаем данные свечи
                quote = candle['quote']['USD']
                
                # Создаем запись для DataFrame
                row = {
                    'timestamp': quote.get('timestamp', candle.get('time_close')),
                    'open': quote.get('open', 0.0),
                    'high': quote.get('high', 0.0),
                    'low': quote.get('low', 0.0),
                    'close': quote.get('close', 0.0),
                    'volume': quote.get('volume', 0.0)
                }
                
                rows.append(row)
            
            # Если нет данных, возвращаем None
            if not rows:
                self.logger.warning(f"В JSON файле не найдено данных в ожидаемом формате для {symbol} ({timeframe})")
                return None
            
            # Создаем DataFrame
            df = pd.DataFrame(rows)
            
            # Преобразуем timestamp в datetime и устанавливаем как индекс
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df.set_index('timestamp', inplace=True)
            
            # Сортируем по индексу (времени)
            df.sort_index(inplace=True)
            
            # Добавляем атрибут с символом монеты
            df.symbol = symbol
            
            # После загрузки данных добавляем расширенную информацию
            if len(df) > 0:
                self.logger.info(f"Данные загружены успешно: {len(df)} строк, столбцы: {list(df.columns)}")
                self.logger.debug(f"Первые 3 строки данных:\n{df.head(3)}")
                self.logger.debug(f"Загрузка данных для {symbol} ({timeframe}) заняла {time.time() - start_time:.4f} секунд")
            
            return df
                
        except Exception as e:
            self.logger.error(f"Ошибка при загрузке данных для {symbol} ({timeframe}): {e}")
            self.logger.debug(traceback.format_exc())
            return None
    
    def analyze_indicator(self, indicator_name: str, df: pd.DataFrame, timeframe: str) -> Dict:
        """
        Анализирует данные с использованием указанного индикатора.
        
        Args:
            indicator_name: Имя индикатора
            df: DataFrame с данными
            timeframe: Таймфрейм
            
        Returns:
            Словарь с результатами анализа
        """
        indicator = self.indicator_registry.get(indicator_name)
        if not indicator:
            return {
                'success': False,
                'error': f"Индикатор {indicator_name} не зарегистрирован"
            }
        
        try:
            # Рассчитываем индикатор
            df_with_indicator = indicator.calculate(df.copy())
            
            # Проводим контекстуальный анализ
            analysis_result = indicator.analyze_context(df_with_indicator, timeframe)
            
            return {
                'success': True,
                'indicator_name': indicator_name,
                'symbol': getattr(df, 'symbol', 'unknown'),
                'timeframe': timeframe,
                'result': analysis_result._asdict(),
                'data_points': len(df)
            }
        except Exception as e:
            self.logger.error(f"Ошибка при анализе индикатора {indicator_name}: {e}")
            self.logger.debug(traceback.format_exc())
            return {
                'success': False,
                'error': str(e),
                'indicator_name': indicator_name,
                'symbol': getattr(df, 'symbol', 'unknown'),
                'timeframe': timeframe
            }
    
    def analyze_timeframe(self, symbol: str, timeframe: str, indicator_list: List[str] = None) -> Dict:
        """
        Анализирует данные для указанного символа и таймфрейма с использованием всех индикаторов.
        
        Args:
            symbol: Символ монеты
            timeframe: Таймфрейм
            indicator_list: Список индикаторов для расчета (если None, используются все доступные)
            
        Returns:
            Словарь с результатами анализа
        """
        start_time = time.time()
        
        try:
            # Загружаем данные
            df = self.load_data(symbol, timeframe)
            if df is None:
                return {
                    'success': False, 
                    'error': 'Не удалось загрузить данные',
                    'symbol': symbol,
                    'timeframe': timeframe
                }
            
            # Если список индикаторов не указан, используем все доступные
            if indicator_list is None:
                indicator_list = list(self.indicator_registry.get_all().keys())
            
            # Анализируем каждый индикатор
            results = {}
            for indicator_name in indicator_list:
                if self.indicator_registry.has_indicator(indicator_name):
                    indicator_result = self.analyze_indicator(indicator_name, df, timeframe)
                    results[indicator_name] = indicator_result
                else:
                    self.logger.warning(f"Индикатор {indicator_name} не найден. Пропускаем.")
            
            execution_time = time.time() - start_time
            
            # Формируем результат
            return {
                'success': True,
                'symbol': symbol,
                'timeframe': timeframe,
                'indicators': results,
                'data_points': len(df) if df is not None else 0,
                'execution_time': execution_time
            }
        except Exception as e:
            self.logger.error(f"Ошибка при анализе {symbol} ({timeframe}): {e}")
            self.logger.debug(traceback.format_exc())
            
            execution_time = time.time() - start_time
            
            return {
                'success': False,
                'error': str(e),
                'symbol': symbol,
                'timeframe': timeframe,
                'execution_time': execution_time
            }
    
    def analyze_symbol(self, symbol: str, timeframes: List[str] = None) -> Dict:
        """
        Анализирует данные для указанного символа по всем таймфреймам.
        
        Args:
            symbol: Символ монеты
            timeframes: Список таймфреймов для анализа (если None, используются все доступные)
            
        Returns:
            Словарь с результатами анализа
        """
        start_time = time.time()
        
        self.logger.info(f"=== Начало анализа монеты {symbol} ===")

        try:
            # Если таймфреймы не указаны, используем все доступные из конфигурации
            if timeframes is None:
                timeframes = [tf for tf, config in self.config.get('TIMEFRAMES', {}).items() 
                             if config.get('enabled', True)]
            
            # Разделяем таймфреймы на важные и вторичные
            primary_timeframes = [tf for tf in timeframes if tf in ['7d', '24h', '4h', '1h']]
            secondary_timeframes = [tf for tf in timeframes if tf in ['30m', '15m', '5m']]
            
            results = {}
            signals_by_tf = {}
            
            # Сначала анализируем основные таймфреймы
            for tf in primary_timeframes:
                result = self.analyze_timeframe(symbol, tf)
                if result['success']:
                    results[tf] = result
                    
                    # Сохраняем результаты анализа
                    self.data_storage.save_indicators(symbol, tf, result)
                    self.logger.info(f"Результаты индикаторов для {symbol} ({tf}) сохранены")
            
            # Определяем, нужно ли анализировать вторичные таймфреймы
            analyze_secondary = self._should_analyze_secondary_timeframes(symbol, results)
            
            # Если нужно, анализируем вторичные таймфреймы
            if analyze_secondary and secondary_timeframes:
                for tf in secondary_timeframes:
                    result = self.analyze_timeframe(symbol, tf)
                    if result['success']:
                        results[tf] = result
                        
                        # Сохраняем результаты анализа
                        self.data_storage.save_indicators(symbol, tf, result)
                        self.logger.info(f"Результаты индикаторов для {symbol} ({tf}) сохранены")
            
            # Определяем сводный сигнал по всем таймфреймам
            overall_signal = self._determine_overall_signal(results)
            
            self.logger.info(f"=== Завершение анализа монеты {symbol} ===")

            execution_time = time.time() - start_time
            
            return {
                'success': True,
                'symbol': symbol,
                'results': results,
                'overall_signal': overall_signal,
                'analyzed_timeframes': list(results.keys()),
                'execution_time': execution_time,
                'secondary_timeframes_analyzed': analyze_secondary
            }
            
        except Exception as e:
            self.logger.error(f"Ошибка при анализе символа {symbol}: {e}")
            self.logger.debug(traceback.format_exc())

            execution_time = time.time() - start_time
            
            return {
                'success': False,
                'error': str(e),
                'symbol': symbol,
                'execution_time': execution_time
            }
        
    def get_multi_timeframe_consensus(self, symbol: str, timeframes: List[str] = None) -> Dict:
        """
        Вычисляет консенсус между разными таймфреймами для индикатора RSI.
        
        Args:
            symbol: Символ монеты
            timeframes: Список таймфреймов для анализа (если None, используются стандартные)
            
        Returns:
            Dict: Результат консенсусного анализа
        """
        if timeframes is None:
            timeframes = ['1h', '4h', '24h', '7d']
        
        # Веса для разных таймфреймов (можно настроить)
        weights = {
            '1h': 0.2,   # 20% - для внутридневной торговли
            '4h': 0.3,   # 30% - для среднесрочных сигналов
            '24h': 0.3,  # 30% - для основного тренда
            '7d': 0.2    # 20% - для исторического контекста
        }
        
        results = {}
        buy_score = 0.0
        sell_score = 0.0
        total_weight = 0.0
        
        # Собираем результаты по всем таймфреймам
        for tf in timeframes:
            if tf not in weights:
                continue
                
            result = self.analyze_timeframe(symbol, tf)
            results[tf] = result
            
            if not result.get('success', False):
                continue
                
            weight = weights[tf]
            total_weight += weight
            
            # Получаем рекомендацию RSI
            if 'indicators' in result and 'RSI' in result['indicators']:
                rsi_result = result['indicators']['RSI']['result']
                recommendation = rsi_result.get('recommendation')
                confidence = rsi_result.get('confidence', 0.5)
                
                if recommendation == SignalType.BUY.value:
                    buy_score += confidence * weight
                elif recommendation == SignalType.SELL.value:
                    sell_score += confidence * weight
        
        # Нормализуем оценки
        if total_weight > 0:
            buy_score /= total_weight
            sell_score /= total_weight
        
        # Определяем общую рекомендацию
        if buy_score > 0.6 and buy_score > sell_score * 1.5:
            consensus = SignalType.BUY.value
            consensus_confidence = buy_score
        elif sell_score > 0.6 and sell_score > buy_score * 1.5:
            consensus = SignalType.SELL.value
            consensus_confidence = sell_score
        else:
            consensus = SignalType.NEUTRAL.value
            consensus_confidence = 0.5
        
        return {
            'success': True,
            'symbol': symbol,
            'consensus': consensus,
            'confidence': consensus_confidence,
            'buy_score': buy_score,
            'sell_score': sell_score,
            'timeframes': results
        }
    
    def _should_analyze_secondary_timeframes(self, symbol: str, primary_results: Dict) -> bool:
        """
        Определяет, нужно ли анализировать вторичные таймфреймы.
        
        Вторичные таймфреймы анализируются, если на первичных таймфреймах наблюдается
        восходящий тренд или потенциально прибыльная ситуация.
        
        Args:
            symbol: Символ монеты
            primary_results: Результаты анализа первичных таймфреймов
            
        Returns:
            True если нужно анализировать вторичные таймфреймы, иначе False
        """
        # Для тестирования просто вернем True
        if self.config.get('testing', {}).get('enabled', False):
            return True
        
        # Проверяем результаты по каждому таймфрейму
        buy_signals_count = 0
        analyzed_count = 0
        
        for tf, result in primary_results.items():
            if not result.get('success', False):
                continue
                
            analyzed_count += 1
            
            # Проверяем каждый индикатор
            for indicator_name, indicator_result in result.get('indicators', {}).items():
                if not indicator_result.get('success', False):
                    continue
                    
                indicator_data = indicator_result.get('result', {})
                recommendation = indicator_data.get('recommendation')
                
                if recommendation == SignalType.BUY.value or recommendation == SignalType.STRONG_BUY.value:
                    buy_signals_count += 1
        
        # Если проанализировано менее 2 таймфреймов, анализируем вторичные
        if analyzed_count < 2:
            return True
        
        # Если более 50% индикаторов дают сигнал на покупку, анализируем вторичные
        return buy_signals_count / analyzed_count >= 0.5
    
    def _determine_overall_signal(self, results: Dict) -> Dict:
        """
        Определяет общий сигнал на основе результатов анализа всех таймфреймов.
        
        Args:
            results: Словарь с результатами анализа по таймфреймам
            
        Returns:
            Словарь с общим сигналом
        """
        # Весовые коэффициенты для разных таймфреймов
        tf_weights = {
            '7d': 0.1,   # 10%
            '24h': 0.3,  # 30%
            '4h': 0.3,   # 30%
            '1h': 0.3,   # 30%
            '30m': 0.0,  # Вторичные таймфреймы не учитываются в общей оценке
            '15m': 0.0,
            '5m': 0.0
        }
        
        buy_score = 0.0
        sell_score = 0.0
        neutral_score = 0.0
        weight_sum = 0.0
        
        for tf, result in results.items():
            if not result.get('success', False):
                continue
                
            tf_weight = tf_weights.get(tf, 0.0)
            
            # Суммируем оценки по всем индикаторам
            for indicator_name, indicator_result in result.get('indicators', {}).items():
                if not indicator_result.get('success', False):
                    continue
                    
                indicator_data = indicator_result.get('result', {})
                recommendation = indicator_data.get('recommendation')
                confidence = indicator_data.get('confidence', 0.5)
                
                # Взвешиваем рекомендацию в зависимости от уверенности
                if recommendation == SignalType.BUY.value or recommendation == SignalType.STRONG_BUY.value:
                    buy_score += confidence * tf_weight
                elif recommendation == SignalType.SELL.value or recommendation == SignalType.STRONG_SELL.value:
                    sell_score += confidence * tf_weight
                else:
                    neutral_score += tf_weight
                
                weight_sum += tf_weight
        
        # Если нет данных, возвращаем нейтральный сигнал
        if weight_sum == 0:
            return {
                'signal': SignalType.NEUTRAL.value,
                'buy_score': 0.0,
                'sell_score': 0.0,
                'neutral_score': 1.0,
                'confidence': 0.5
            }
        
        # Нормализуем оценки
        buy_score /= weight_sum
        sell_score /= weight_sum
        neutral_score /= weight_sum
        
        # Определяем итоговый сигнал
        if buy_score > sell_score and buy_score > neutral_score and buy_score > 0.6:
            signal = SignalType.BUY.value
            confidence = buy_score
        elif sell_score > buy_score and sell_score > neutral_score and sell_score > 0.6:
            signal = SignalType.SELL.value
            confidence = sell_score
        else:
            signal = SignalType.NEUTRAL.value
            confidence = max(0.5, max(buy_score, sell_score))
        
        return {
            'signal': signal,
            'buy_score': round(buy_score, 2),
            'sell_score': round(sell_score, 2),
            'neutral_score': round(neutral_score, 2),
            'confidence': round(confidence, 2)
        }
    
    def process_symbols(self, symbols: List[str], timeframes: List[str] = None) -> Dict:
        """
        Обрабатывает список символов параллельно.
        
        Args:
            symbols: Список символов для обработки
            timeframes: Список таймфреймов для анализа (если None, используются все доступные)
            
        Returns:
            Словарь с результатами обработки
        """
        def process_symbol_batch(symbol_batch):
            batch_results = {
                'successful_symbols': 0,
                'failed_symbols': 0,
                'results': {}
            }
            
            for symbol in symbol_batch:
                try:
                    result = self.analyze_symbol(symbol, timeframes)
                    batch_results['results'][symbol] = result
                    
                    if result.get('success', False):
                        batch_results['successful_symbols'] += 1
                    else:
                        batch_results['failed_symbols'] += 1
                except Exception as e:
                    self.logger.error(f"Ошибка при обработке символа {symbol}: {e}")
                    self.logger.debug(traceback.format_exc())
                    batch_results['failed_symbols'] += 1
            
            return batch_results
        
        # Обрабатываем символы параллельно
        results = self.parallel_processor.process_symbols(symbols, process_symbol_batch)
        
        # Находим топ символы на основе сигналов
        top_symbols = self._get_top_symbols(results.get('results', {}))
        results['top_symbols'] = top_symbols
        
        return results
    
    def _get_top_symbols(self, results: Dict, limit: int = 10) -> List[Dict]:
        """
        Находит топ символы на основе результатов анализа.
        
        Args:
            results: Словарь с результатами анализа по символам
            limit: Максимальное количество символов в результате
            
        Returns:
            Список словарей с информацией о топ символах
        """
        symbols_data = []
        
        for symbol, result in results.items():
            if not result.get('success', False):
                continue
                
            overall_signal = result.get('overall_signal', {})
            signal = overall_signal.get('signal', SignalType.NEUTRAL.value)
            confidence = overall_signal.get('confidence', 0.0)
            
            # Отбираем только символы с сигналом покупки и высокой уверенностью
            if signal == SignalType.BUY.value and confidence >= 0.7:
                symbols_data.append({
                    'symbol': symbol,
                    'signal': signal,
                    'confidence': confidence,
                    'buy_score': overall_signal.get('buy_score', 0.0),
                    'timeframes': result.get('analyzed_timeframes', [])
                })
        
        confidence_threshold = 0.7  # Это стандартный порог, используемый в коде
        self.logger.info(f"Порог уверенности для топовых монет: {confidence_threshold}. Найдено монет выше порога: {len(symbols_data)}")

        # Сортируем по уверенности (от высокой к низкой)
        symbols_data.sort(key=lambda x: x['confidence'], reverse=True)
        
        # Ограничиваем количество символов
        return symbols_data[:limit]
    
    def run(self, symbols: List[str] = None, timeframes: List[str] = None) -> Dict:
        """
        Запускает полный процесс анализа индикаторов.
        
        Args:
            symbols: Список символов для анализа (если None, будет загружен из данных)
            timeframes: Список таймфреймов для анализа (если None, используются все доступные)
            
        Returns:
            Словарь с результатами анализа
        """
        self.logger.info("Запуск полного анализа индикаторов")
        start_time = time.time()
        
        # Если символы не указаны, загружаем их
        if symbols is None:
            symbols = self._load_symbols()
            self.logger.info(f"Загружено {len(symbols)} символов для анализа")
        
        # Если таймфреймы не указаны, используем все доступные
        if timeframes is None:
            timeframes = [tf for tf, config in self.config.get('TIMEFRAMES', {}).items() 
                         if config.get('enabled', True)]
            self.logger.info(f"Используем таймфреймы: {', '.join(timeframes)}")
        
        # Запускаем процесс обработки
        results = self.process_symbols(symbols, timeframes)
        
        execution_time = time.time() - start_time
        results['execution_time'] = execution_time
        
        # Логируем результаты
        self.logger.info(f"Анализ завершен за {execution_time:.2f} сек. "
                         f"Успешно: {results.get('successful_symbols', 0)}/{len(symbols)} символов")
        
        # Отправляем уведомление в Telegram, если включено
        if self.config.get('telegram', {}).get('enabled', False):
            self._send_telegram_notification(results)
        
        return results
    
    def _load_symbols(self) -> List[str]:
        """
        Загружает список символов для анализа.
        
        Returns:
            Список символов
        """
        # Для тестирования или если указан фиксированный список символов в конфигурации
        if self.config.get('testing', {}).get('enabled', False):
            return self.config.get('testing', {}).get('symbols', ['BTC', 'ETH', 'BNB', 'SOL', 'XRP'])
        
        symbols = []
        
        try:
            # Определяем директорию с данными для основного таймфрейма
            main_timeframe = '1h'
            data_dir = os.path.join(self.config['paths'].get('DATA_ENGINE', {}).get('current', 'data/brain/data_engine/current'), main_timeframe)
            
            # Проверяем наличие директории
            if not os.path.exists(data_dir):
                self.logger.warning(f"Директория с данными не найдена: {data_dir}")
                return ['BTC', 'ETH', 'BNB', 'SOL', 'XRP']  # Возвращаем базовый список
            
            # Сканируем директорию и извлекаем символы из имен файлов
            for filename in os.listdir(data_dir):
                if filename.endswith('.json'):
                    symbol = filename.split('_')[0]
                    symbols.append(symbol)
            
            self.logger.info(f"Загружено {len(symbols)} символов из директории {data_dir}")
            
        except Exception as e:
            self.logger.error(f"Ошибка при загрузке списка символов: {e}")
            self.logger.debug(traceback.format_exc())
            return ['BTC', 'ETH', 'BNB', 'SOL', 'XRP']  # Возвращаем базовый список в случае ошибки
        
        # Если список пуст, возвращаем базовый список
        if not symbols:
            self.logger.warning("Список символов пуст. Используем базовый список.")
            return ['BTC', 'ETH', 'BNB', 'SOL', 'XRP']
        
        return symbols
    
    def _send_telegram_notification(self, results: Dict) -> None:
        """
        Отправляет уведомление в Telegram с результатами анализа.
        
        Args:
            results: Словарь с результатами анализа
        """
        try:
            # Формируем сообщение
            top_symbols = results.get('top_symbols', [])
            
            message_parts = [
                "📊 *Анализ индикаторов завершен*",
                f"⏱️ Время выполнения: {results.get('execution_time', 0):.2f} сек",
                f"✅ Успешно обработано: {results.get('successful_symbols', 0)}/{results.get('total_symbols', 0)} монет",
                f"📈 Найдено потенциальных монет: {len(top_symbols)}"
            ]
            
            # Добавляем информацию о топ символах
            if top_symbols:
                message_parts.append("\n*Топ монеты:*")
                for i, symbol_data in enumerate(top_symbols[:5], 1):  # Ограничиваем 5 монетами
                    message_parts.append(
                        f"{i}. *{symbol_data['symbol']}* - "
                        f"Уверенность: {symbol_data['confidence']:.2f}, "
                        f"Сигнал: {symbol_data['signal']}"
                    )
            
            message = "\n".join(message_parts)
            
            # Отправляем сообщение
            send_telegram(message, priority="normal")
            self.logger.info("Уведомление в Telegram отправлено")
            
        except Exception as e:
            self.logger.error(f"Ошибка при отправке уведомления в Telegram: {e}")
            self.logger.debug(traceback.format_exc())


# ============================================================
# ФУНКЦИЯ ТЕСТИРОВАНИЯ
# ============================================================

def test_indicator_engine():
    """
    Запускает тестовый анализ индикаторов.
    """
    logger.info("Запуск тестового анализа индикаторов")
    
    # Загружаем конфигурацию для IndicatorEngine
    config = {
        'paths': PATHS,
        'TIMEFRAMES': TIMEFRAMES,
        'processing': PROCESSING,
        'indicators': INDICATORS,
        'testing': {
            'enabled': True,
            'symbols': ['BTC', 'ETH', 'BNB', 'SOL', 'XRP']
        },
        'telegram': {
            'enabled': True  # Отключаем для тестирования
        }
    }
    
    # Создаем экземпляр IndicatorEngine
    engine = IndicatorEngine(config)
    
    # Тестовые монеты
    test_symbols = ['BTC', 'ETH', 'DOGE', 'AI16Z', 'SOL']
    
    # Ограничиваем таймфреймы для тестирования
    test_timeframes = ['1h', '4h', '24h', '7d']
    
    # Запускаем анализ
    results = engine.run(test_symbols, test_timeframes)
    
    # Выводим результаты
    logger.info(f"Тестовый анализ завершен. Успешно обработано: {results.get('successful_symbols', 0)}/{len(test_symbols)} монет")
    
    # Проверяем топ символы
    top_symbols = results.get('top_symbols', [])
    if top_symbols:
        logger.info(f"Топ символы: {', '.join([s['symbol'] for s in top_symbols])}")
    else:
        logger.info("Топ символы не найдены")
    
    return results


if __name__ == "__main__":
    print("Запуск тестового анализа индикаторов")
    
    # Создаем директории, если они не существуют
    indicators_dir = os.path.join(PATHS.get('BRAIN_DATA', 'data/brain/'), "indicators", "current", "1h")
    os.makedirs(indicators_dir, exist_ok=True)
    
    # Запускаем тест
    test_results = test_indicator_engine()
    
    print("Тестовый анализ завершен")
